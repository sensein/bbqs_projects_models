{
  "projects": [
    {
      "award number": "R34DA059510",
      "Type": "R34",
      "Link": "RFA-DA-23-030",
      "#": "DA23-030",
      "PoP years (max)": "2",
      "Title": "A modeling framework and arena for measuring contextual influences of behavior",
      "Award #": "R34DA059510",
      "PI Name": "MCGRATH, PATRICK T",
      "PI Email": "patrick.mcgrath@biology.gatech.edu",
      "Human / Animal": "Animal",
      "Species (Regular Name)": "Cichlid",
      "Species (Scientific Name)": "Cichlidae",
      "Sensors": "Multi-view cameras",
      "Data Modalities": "Video data, Multi-view video",
      "Approaches": "Differentiation of species, Understanding multi-animal interactions, Pose tracking, Object detection, Feature extraction techniques",
      "Sensors (Standardized)": "video cameras",
      "Sensors\n(Dropdown)": "video cameras",
      "Data Modalities (Standardized)": "visual",
      "Data Modalities \n(Dropdown)": "visual",
      "Approaches (Standardized)": "pose estimation; object detection; feature extraction; behavior tracking; graph modeling; self-supervised learning",
      "Approaches \n(Dropdown)": "pose estimation, object detection, feature extraction, behavior tracking, graph modeling, self-supervised learning",
      "Data Generation Start": "9/1/24",
      "Proposal objective": "\"This grant will propose to create a behavioral arena capable of mimicking natural environments that are required for social reproductive behaviors, including interactions between a large number of conspecifics, environmental factors such as male displays, and contextual data such as hierarchical status between various males. \"",
      "Build hardware?": "Yes",
      "Hardware type": "Arena",
      "Hardware details": "build open arena (500 gallon) to support large numbers of fish in naturalistic settings; multi-view livestream; underwater housing",
      "Build software?": "Yes",
      "Software type": "ML model, Tracking, Feature detection, Computer vision, Dimensionality reduction, Pose estimation",
      "Software details": "self-supervised model for behaviour to extract multiple timescales of multi-animal behaviour; pose tracking; bootstrap across multiple scales to learn a short term and long term embedding; graph modeling interactions across animals and contextual information",
      "DMSP software": "manual annotation for object and feature detection -> computer vision networks for automated analysis (pose estimation, object detection and classification, and identification of behavioral signatures)\nlower dimensional representations of raw data",
      "Collect data?": "Yes",
      "Data type": "Behavioral, Environmental, Social interaction, Neural, Annotation",
      "Data details": "underwater camera - 3d behaviours; deviation of sand by depth sensing, changes to environment; iEG",
      "DMSP data": "primary raw data: daily multi-angle video recordings of large social groups (~30 fish) freely behaving in a naturalistic setting for extended (>10 day) periods. manual annotations. continuous 3D pose trajectories; ethograms of individual fish behaviors",
      "Challenges (kickoff videos)": "1.\tBuilding an arena (tank) to support more fish and capture their behavior using a multi-view camera setup\n2.\tPose tracking in fish (seems deeplabcut is good for this)\n3.\tCapture neural activity in fish (future)",
      "Proposed WGs": NaN,
      "Which types of data are you collecting for BBQS?": "Behavioral data (e.g., video recordings, motion capture, eye tracking, gait analysis);Genetic data (e.g., DNA sequencing, gene expression profiles)\t",
      "Which types of data are you collecting for BBQS?\n(Dropdown)": "Behavioral data, Genetic data",
      "What types of behavior are you interested in?": "Locomotion and Movement (for example: walking, running, swimming, climbing, jumping);Social Interactions (for example: grooming, play behavior, aggression, mating behavior);Feeding and Drinking (for example: eating patterns, drinking frequency, food preference);Exploratory Behavior (for example: novel object interaction, maze navigation, environmental exploration);Emotional and Stress Responses (for example: anxiety-related behaviors, fear responses, stress-induced behaviors);Reproductive Behaviors (for example: courtship rituals, nest building, parental care)\t",
      "What types of behavior are you interested in? \n(high level)": "Locomotion and Movement, Social Interactions, Feeding and Drinking, Exploratory Behavior, Emotional and Stress Responses, Reproductive Behaviors\t",
      "What types of behavior are you interested in? \n(high level; dropdown)": "Locomotion and Movement, Social Interactions, Feeding and Drinking, Exploratory Behavior, Emotional and Stress Responses, Reproductive Behaviors\t",
      "What statistical or computational methods do you primarily rely on for your data analysis?": NaN,
      "What types of analyses are you performing on your neural and behavioral data?": "Statistical analysis, Dimensionality reduction, Dynamical systems modeling, Encoding models, Decoding models\t"
    },
    {
      "award number": "R34DA059509",
      "Type": "R34",
      "Link": "RFA-DA-23-030",
      "#": "DA23-030",
      "PoP years (max)": "2",
      "Title": "Behavioral quantification through active learning and multidimensional physiological monitoring",
      "Award #": "R34DA059509",
      "PI Name": "YTTRI, ERIC",
      "PI Email": "eyttri@andrew.cmu.edu",
      "Human / Animal": "Animal",
      "Species (Regular Name)": "Mouse",
      "Species (Scientific Name)": "Mus musculus",
      "Sensors": "Neuropixels, Selfie camera, Thermocam, Ultrasonic microphones",
      "Data Modalities": "Neural, Video data, Audio",
      "Approaches": "Analysis of changes in response to stress (intruder, thirst) and gut brain axis",
      "Sensors (Standardized)": "neuropixels; thermal cameras; video cameras; ultrasonic microphones; RNA sequencing; heart rate sensors; eye tracker",
      "Sensors\n(Dropdown)": "neuropixels, thermal cameras, ultrasonic microphones, RNA sequencing, heart rate sensors, eye tracker",
      "Data Modalities (Standardized)": "neural; visual; audio; molecular",
      "Data Modalities \n(Dropdown)": "neural, visual, molecular",
      "Approaches (Standardized)": "pose estimation; physiological signal analysis; speech analysis; behavior tracking",
      "Approaches \n(Dropdown)": "pose estimation, physiological signal analysis, speech analysis, behavior tracking",
      "Data Generation Start": "1/1/25",
      "Proposal objective": "\"We seek to expand the capabilities of our current behavioral segmentation approaches to provide a more precise and comprehensive account of behavior. \"",
      "Build hardware?": "Yes",
      "Hardware type": "Arena",
      "Hardware details": "build open arena (500 gallon) to support large numbers of fish in naturalistic settings; multi-view livestream; underwater housing",
      "Build software?": "Yes",
      "Software type": "Analysis, End product, Benchmark, Behavioral segmentation, Behavioral quantification, Pose estimation",
      "Software details": "intuitive GUI, expand behaviourl classification pipeline to quantify complex and multimodal dynamics; benchmark pipeline; include non-pose signals into classifier",
      "DMSP software": "pose estimation via SLEAP; neural data KiloSort3",
      "Collect data?": "Yes",
      "Data type": "Physiological, Behavioral, Video, Neural",
      "Data details": "Behavioural dynamics at multiple timescales; pose; pupil, physiology, vocalisations; urination; HR, body temperature, gut microbiota; single-animal intruder assay; resource scarcity assay (water)",
      "DMSP data": "primary raw data: video (color, infrared, thermal); audio; neural data. rRNA seq data from fecal samples",
      "Challenges (kickoff videos)": "1.\tSpoke about some tools they built that could be helpful to other researchers\n2.\tMany physiological signals (specify them here) are collected at different scales, rates, and dimensionality. (4TB per animal per day)\n3.\tProbably A-SOID can be used by other researchers interested in characterizing social behaviors",
      "Proposed WGs": "Methods",
      "Which types of data are you collecting for BBQS?": "Behavioral data (e.g., video recordings, motion capture, eye tracking, gait analysis);Genetic data (e.g., DNA sequencing, gene expression profiles)\t",
      "Which types of data are you collecting for BBQS?\n(Dropdown)": "Behavioral data, Genetic data",
      "What types of behavior are you interested in?": "Locomotion and Movement (for example: walking, running, swimming, climbing, jumping);Social Interactions (for example: grooming, play behavior, aggression, mating behavior);Feeding and Drinking (for example: eating patterns, drinking frequency, food preference);Exploratory Behavior (for example: novel object interaction, maze navigation, environmental exploration);Emotional and Stress Responses (for example: anxiety-related behaviors, fear responses, stress-induced behaviors);Reproductive Behaviors (for example: courtship rituals, nest building, parental care)\t",
      "What types of behavior are you interested in? \n(high level)": "Locomotion and Movement, Social Interactions, Feeding and Drinking, Exploratory Behavior, Emotional and Stress Responses, Reproductive Behaviors\t",
      "What types of behavior are you interested in? \n(high level; dropdown)": "Locomotion and Movement, Social Interactions, Feeding and Drinking, Exploratory Behavior, Emotional and Stress Responses, Reproductive Behaviors\t",
      "What statistical or computational methods do you primarily rely on for your data analysis?": NaN,
      "What types of analyses are you performing on your neural and behavioral data?": "Statistical analysis, Dimensionality reduction, Dynamical systems modeling, Encoding models, Decoding models\t"
    },
    {
      "award number": "R34DA059513",
      "Type": "R34",
      "Link": "RFA-DA-23-030",
      "#": "DA23-030",
      "PoP years (max)": "2",
      "Title": "Computational attribution and fusion of vocalizations, social behavior, and neural recordings in a naturalistic environment",
      "Award #": "R34DA059513",
      "PI Name": "WILLIAMS, ALEXANDER HENRY",
      "PI Email": "ahwillia@stanford.edu",
      "Human / Animal": "Animal",
      "Species (Regular Name)": "Gerbil",
      "Species (Scientific Name)": "Meriones unguiculatus",
      "Sensors": "Ultrasonic Microphones, Infrared Cameras, Wireless neural recorders",
      "Data Modalities": "Wireless neural recordings",
      "Approaches": "Machine learning, Pose tracking, Vocalizations",
      "Sensors (Standardized)": "ultrasonic microphones; infrared cameras; wireless neural; IMU",
      "Sensors\n(Dropdown)": "ultrasonic microphones, microphones, infrared cameras, wireless neural, IMU",
      "Data Modalities (Standardized)": "audio; kinematic; neural",
      "Data Modalities \n(Dropdown)": "audio, kinematic, neural",
      "Approaches (Standardized)": "speech analysis; source localization; pose estimation; behavior tracking; deep learning",
      "Approaches \n(Dropdown)": "speech analysis, source localization, pose estimation, behavior tracking, deep learning",
      "Data Generation Start": "6/1/24",
      "Proposal objective": "\"This proposal will develop novel experimental and computational methods to attribute vocal and non-vocal sounds to individuals in a naturalistic, acoustically complex, multi-animal environment. \"",
      "Build hardware?": "Yes",
      "Hardware type": "Recording device",
      "Hardware details": "Large, naturalistic environment with simultaneous camera and microphone array recordings",
      "Build software?": "Yes",
      "Software type": "Benchmark, ML model, Prediction, Analysis, Regression model, Pose estimation, Tracking, Behavioral segmentation",
      "Software details": "Model how neurons allow behavior to adapt to constraints of physical environment. ",
      "DMSP software": "video: SLEAP to extract timescales of each animal's body pose by tracking key anatomical points -> low-dim time series of 2D/3D joint positions. open source python. ",
      "Collect data?": "Yes",
      "Data type": "Audio, Social interaction, Behavioral, Neural, Visual, Multi-modal, Animal, Wearable sensor, \"Invasive (e.g. single unit, ECoG)\", Kinematic",
      "Data details": "vocal & non-vocal sounds; multi-animal interactions; naturalistic environment; simultaneous camera and microphone; continuous wireless electrophysiological recording ",
      "DMSP data": "Multi-modal behavioral and neural data from multi-animal environments. \nprimary raw data: video recordings (multiple cameras, different angles), audio recordings (multiple microphones different locations); extracellular voltage recordings (auditory cortex); accelerometer and gyroscope from inertial measurement units (IMUs) implanted on the headstage to capture finescale head movements.\nFiner-scale motor kinematics (high-res EMG)",
      "Challenges (kickoff videos)": "1.        Collection of vocal information from Mongolian gerbils including video capture (body pose estimation).  \n2.        Incorporating physiology and behavior with vocal and video capture.\n3.        Techniques/Methods for source localization in complex and reverberant environments (difficult to quantify uncertainty)",
      "Proposed WGs": "Deep Learning",
      "Which types of data are you collecting for BBQS?": "Neural data (e.g., EEG, MEG, fMRI, ECoG, single-unit recordings);Behavioral data (e.g., video recordings, motion capture, eye tracking, gait analysis);Cognitive performance data (e.g., reaction times, accuracy, task performance metrics);Environmental data (e.g., ambient light, temperature, sound levels);Social interaction data (e.g., proximity sensors, communication logs);Wearable sensor data (e.g., accelerometers, gyroscopes, smartwatches)\t",
      "Which types of data are you collecting for BBQS?\n(Dropdown)": "Neural data, Behavioral data, Cognitive performance data, Environmental data, Social interaction data, Wearable sensor data",
      "What types of behavior are you interested in?": "Locomotion and Movement (for example: walking, running, swimming, climbing, jumping);Gait Analysis (for example: stride length, step frequency, limb coordination);Vocalizations and Speech (for example: talking, crying, barking, chirping);Social Interactions (for example: grooming, play behavior, aggression, mating behavior);Exploratory Behavior (for example: novel object interaction, maze navigation, environmental exploration);Emotional and Stress Responses (for example: anxiety-related behaviors, fear responses, stress-induced behaviors);Physical Activity Levels (for example: activity counts, sedentary behavior, exercise routines);Reproductive Behaviors (for example: courtship rituals, nest building, parental care)\t",
      "What types of behavior are you interested in? \n(high level)": "Locomotion and Movement, Gait Analysis, Vocalizations and Speech, Social Interactions, Exploratory Behavior, Emotional and Stress Responses, Physical Activity Levels, Reproductive Behaviors\t",
      "What types of behavior are you interested in? \n(high level; dropdown)": "Locomotion and Movement, Gait Analysis, Vocalizations and Speech, Social Interactions, Exploratory Behavior, Emotional and Stress Responses, Physical Activity Levels, Reproductive Behaviors\t",
      "What statistical or computational methods do you primarily rely on for your data analysis?": "Classical statistical methods (e.g., t-tests, ANOVA);Time series analysis;Unsupervised machine learning (e.g., clustering, PCA);Supervised machine learning (e.g., classification, regression);Deep learning (e.g., neural networks, CNNs, RNNs)\t",
      "What types of analyses are you performing on your neural and behavioral data?": "Statistical analysis, Signal processing, Dimensionality reduction, Dynamical systems modeling, Encoding models, Decoding models, Correlation analysis, Regression models        "
    },
    {
      "award number": "R34DA059507",
      "Type": "R34",
      "Link": "RFA-DA-23-030",
      "#": "DA23-030",
      "PoP years (max)": "2",
      "Title": "Development of a smart aviary to probe neural dynamics of complex social behaviors in a gregarious songbird",
      "Award #": "R34DA059507",
      "PI Name": "SCHMIDT, MARC F",
      "PI Email": "marcschm@sas.upenn.edu",
      "Human / Animal": "Animal",
      "Species (Regular Name)": "Cowbird",
      "Species (Scientific Name)": "Molothrus",
      "Sensors": "Wireless Neural recorder, RFID, Microphone arrays, Cameras",
      "Data Modalities": "Wireless neural recording",
      "Approaches": "Social network analysis, Camera tracking",
      "Sensors (Standardized)": "video cameras; microphones; RFID; wireless neural",
      "Sensors\n(Dropdown)": "video cameras, microphones, RFID, wireless neural",
      "Data Modalities (Standardized)": "visual; physiological; neural; spatial",
      "Data Modalities \n(Dropdown)": "visual, physiological, neural, spatial",
      "Approaches (Standardized)": "object detection; pose estimation; speech analysis; source localization; behavior tracking; social network analysis",
      "Approaches \n(Dropdown)": "object detection, pose estimation, speech analysis, source localization, behavior tracking, social network analysis",
      "Data Generation Start": "4/1/25",
      "Proposal objective": "\"we propose to study the brown-headed cowbird (Molothrus ater), a highly gregarious songbird species whose social behavior has been well studied and where vocal and non-vocal communication signals form a central and critical component of its social system.",
      "Build hardware?": "Yes",
      "Hardware type": "Testing room, Recording device",
      "Hardware details": "\"smart aviary\"  with cameras and microphones to minotor behaviour in each individual; wireless neural recording device coupled to carbon fiber electrodes",
      "Build software?": "Yes",
      "Software type": "Computer vision, Analysis, Tracking, Behavioral segmentation, Behavioral quantification, ML model",
      "Software details": "a fully automated system using computer vision and machine learning technology to evaluate moment-to- moment behavioral interactions",
      "DMSP software": "training/testing for neural networks",
      "Collect data?": "Yes",
      "Data type": "Audio, Behavioral, Visual, Video, Neural , \"Invasive (e.g. single unit, ECoG)\", Environmental, Annotation, Animal",
      "Data details": "location, tracking, pose, sound, types of sound and where its produced, social context; wireless neural activity",
      "DMSP data": "primary raw data: video (machine vision cameras), audio, electrophysiological data, temperature and humidity from environmental sensors.\n3D position, orientation, pose trajectories; detected vocalizations.\ndatabase of behavioral events (ethograms); annotations",
      "Challenges (kickoff videos)": "1.        Only bird project in the consortium\n2.        Video and audio recording of the birds in the aviary. Need sound source localization. See Project 07 (PI Schneider). (Potential collaboration between P11 (PI Schoppik) and P7 there?)\n3.        Instrumentation (like a bird hat) for recording neural activity",
      "Proposed WGs": "Deep Learning",
      "Which types of data are you collecting for BBQS?": "Neural data (e.g., EEG, MEG, fMRI, ECoG, single-unit recordings);Behavioral data (e.g., video recordings, motion capture, eye tracking, gait analysis);Social interaction data (e.g., proximity sensors, communication logs);Wearable sensor data (e.g., accelerometers, gyroscopes, smartwatches)",
      "Which types of data are you collecting for BBQS?\n(Dropdown)": "Neural data, Behavioral data, Social interaction data, Wearable sensor data",
      "What types of behavior are you interested in?": "Locomotion and Movement (for example: walking, running, swimming, climbing, jumping);Posture and Balance (for example: standing, sitting, lying down, postural sway);Vocalizations and Speech (for example: talking, crying, barking, chirping);Social Interactions (for example: grooming, play behavior, aggression, mating behavior);Reproductive Behaviors (for example: courtship rituals, nest building, parental care)\t",
      "What types of behavior are you interested in? \n(high level)": "Locomotion and Movement, Posture and Balance, Vocalizations and Speech, Social Interactions, Reproductive Behaviors\t",
      "What types of behavior are you interested in? \n(high level; dropdown)": "Locomotion and Movement, Posture and Balance, Vocalizations and Speech, Social Interactions, Reproductive Behaviors\t",
      "What statistical or computational methods do you primarily rely on for your data analysis?": "Classical statistical methods (e.g., t-tests, ANOVA);Time series analysis;Unsupervised machine learning (e.g., clustering, PCA);Supervised machine learning (e.g., classification, regression);Deep learning (e.g., neural networks, CNNs, RNNs)\t",
      "What types of analyses are you performing on your neural and behavioral data?": "Dimensionality reduction\t"
    },
    {
      "award number": "R34DA059718",
      "Type": "R34",
      "Link": "RFA-DA-23-030",
      "#": "DA23-030",
      "PoP years (max)": "2",
      "Title": "Harnessing biological rhythms for a resilient social motif generator",
      "Award #": "R34DA059718",
      "PI Name": "WESSON, DANIEL W",
      "PI Email": "danielwesson@ufl.edu",
      "Human / Animal": "Animal",
      "Species (Regular Name)": "Mouse",
      "Species (Scientific Name)": "Mus musculus",
      "Sensors": "Respiration Sensors, Heart rate Monitors, accelerometers",
      "Data Modalities": "Respiration signals, Heart rate, High-speed video",
      "Approaches": "Modeling social behavior from video and physiological rhythms",
      "Sensors (Standardized)": "video cameras; respiration sensors; heart rate sensors; accelerometer",
      "Sensors\n(Dropdown)": "video cameras, respiration sensors, heart rate sensors, accelerometer",
      "Data Modalities (Standardized)": "visual; physiological; kinematic",
      "Data Modalities \n(Dropdown)": "visual, physiological, kinematic",
      "Approaches (Standardized)": "pose estimation; physiological signal analysis; deep learning; clustering; behavior tracking",
      "Approaches \n(Dropdown)": "pose estimation, physiological signal analysis, deep learning, clustering, behavior tracking",
      "Data Generation Start": "9/2/24",
      "Proposal objective": "\"build artificial intelligence (AI) tools that are capable of integrating multivariate sources of behavior data to quantify spatiotemporal signatures or \u201cmotifs\u201d of diverse repertoires of social behaviors\"",
      "Build hardware?": "Yes",
      "Hardware type": "Arena",
      "Hardware details": "Experiment (camera + sensors)",
      "Build software?": "Yes",
      "Software type": "Computer vision, ML model, End product",
      "Software details": "hierarchical multi-timescale model of social behavioral motifs; multimodal autoencoder; dynamical hierarchical clustering",
      "DMSP software": "to develop So-Mo for social motif generation; neural networks",
      "Collect data?": "Yes",
      "Data type": "Animal, Wearable sensor, Behavioral, Video, Physiological, Multi-modal",
      "Data details": "Concurrent autonomic rhythms (breathing, HR), movement from head-mounted accelerometer, physiological",
      "DMSP data": "primary raw data: sensor data, video files of behavioral recordings (one camera per session, homecage and experiments), breathing data, accelerometer, heartrate. ",
      "Challenges (kickoff videos)": "1.\tCapture behavioral motifs using respiration and heart rate. Of course, there is video as well\n2.\tDeep learning  methodological developments\n3.\tThere is no audio  in this project.",
      "Proposed WGs": "Deep learning",
      "Which types of data are you collecting for BBQS?": "Behavioral data (e.g., video recordings, motion capture, eye tracking, gait analysis);Physiological data (e.g., heart rate, skin conductance, respiratory rate);Wearable sensor data (e.g., accelerometers, gyroscopes, smartwatches)\t\t\t",
      "Which types of data are you collecting for BBQS?\n(Dropdown)": "Behavioral data, Physiological data, Wearable sensor data",
      "What types of behavior are you interested in?": "Social Interactions (for example: grooming, play behavior, aggression, mating behavior)\t\t",
      "What types of behavior are you interested in? \n(high level)": "Social Interactions\t\t",
      "What types of behavior are you interested in? \n(high level; dropdown)": "Social Interactions\t\t",
      "What statistical or computational methods do you primarily rely on for your data analysis?": "Classical statistical methods (e.g., t-tests, ANOVA);Time series analysis;Unsupervised machine learning (e.g., clustering, PCA);Supervised machine learning (e.g., classification, regression);Deep learning (e.g., neural networks, CNNs, RNNs)\t",
      "What types of analyses are you performing on your neural and behavioral data?": "Statistical analysis, Dimensionality reduction, Dynamical systems modeling, Encoding models, Decoding models, Regression models\t"
    },
    {
      "award number": "R34DA059506",
      "Type": "R34",
      "Link": "RFA-DA-23-030",
      "#": "DA23-030",
      "PoP years (max)": "2",
      "Title": "High-resolution 3D tracking of social behaviors for deep phenotypic analysis",
      "Award #": "R34DA059506",
      "PI Name": "OLVECZKY, BENCE P",
      "PI Email": "olveczky@fas.harvard.edu",
      "Human / Animal": "Animal",
      "Species (Regular Name)": "Rats, Mouse",
      "Species (Scientific Name)": "Rattus, Mus musculus",
      "Sensors": "Synchronized cameras",
      "Data Modalities": "Video data",
      "Approaches": "Deep neural network, 3D pose estimation",
      "Sensors (Standardized)": "video cameras",
      "Sensors\n(Dropdown)": "video cameras",
      "Data Modalities (Standardized)": "kinematic; visual",
      "Data Modalities \n(Dropdown)": "kinematic, visual",
      "Approaches (Standardized)": "pose estimation; deep learning; behavior tracking",
      "Approaches \n(Dropdown)": "pose estimation, deep learning, behavior tracking",
      "Data Generation Start": "12/1/24",
      "Proposal objective": "\"to plan for and deliver a proof-of-concept solution for an innovative and easy-to-use experimental platform for measuring and quantifying social behaviors in animal models. \"",
      "Build hardware?": "Yes",
      "Hardware type": "Arena",
      "Hardware details": "\"Experimental platform\" for social behaviors",
      "Build software?": "Yes",
      "Software type": "Behavioral quantification, Behavioral segmentation, Tracking, End product, ML model, Pose estimation",
      "Software details": "deep neural network trained across images from synchronized cameras; infer coordiates of multiple animals",
      "DMSP software": "open-source python; parse behavior into modules; behavior tracking system; deep network for 3D kinematic tracking",
      "Collect data?": "Yes",
      "Data type": "Animal, Social interaction, Video, Behavioral, Visual, Kinematic",
      "Data details": "Precise, whole body kinematics, 3D kinematics during social behaviors; rats and mice",
      "DMSP data": "high-resolution behavior; hundreds of hours of video and 3D kinematic traces of interacting rats; social behavior annotations for all frames of video",
      "Challenges (kickoff videos)": "1.\tCapture behavioral motifs using respiration and heart rate. Of course, there is video as well\n2.\tDeep learning  methodological developments\n3.\tThere is no audio  in this project.",
      "Proposed WGs": "Deep learning",
      "Which types of data are you collecting for BBQS?": "Behavioral data (e.g., video recordings, motion capture, eye tracking, gait analysis);Social interaction data (e.g., proximity sensors, communication logs)",
      "Which types of data are you collecting for BBQS?\n(Dropdown)": "Behavioral data, Social interaction data",
      "What types of behavior are you interested in?": "Locomotion and Movement (for example: walking, running, swimming, climbing, jumping);Gait Analysis (for example: stride length, step frequency, limb coordination);Posture and Balance (for example: standing, sitting, lying down, postural sway);Social Interactions (for example: grooming, play behavior, aggression, mating behavior);Exploratory Behavior (for example: novel object interaction, maze navigation, environmental exploration)                ",
      "What types of behavior are you interested in? \n(high level)": "Social Interactions\t\t",
      "What types of behavior are you interested in? \n(high level; dropdown)": "Locomotion and Movement, Gait Analysis, Posture and Balance, \nSocial Interactions, Exploratory Behavior",
      "What statistical or computational methods do you primarily rely on for your data analysis?": "Classical statistical methods (e.g., t-tests, ANOVA);Time series analysis;Unsupervised machine learning (e.g., clustering, PCA);Supervised machine learning (e.g., classification, regression);Deep learning (e.g., neural networks, CNNs, RNNs)\t",
      "What types of analyses are you performing on your neural and behavioral data?": "Statistical analysis, Signal processing, Dimensionality reduction, Dynamical systems modeling, Time-frequency analysis, Bayesian inference, Encoding models, Decoding models, Correlation analysis, Regression models        "
    },
    {
      "award number": "R34DA059512",
      "Type": "R34",
      "Link": "RFA-DA-23-030",
      "#": "DA23-030",
      "PoP years (max)": "2",
      "Title": "High-throughput, high-resolution 3D measurement of ethologically relevant rodent behavior in a dynamic environment",
      "Award #": "R34DA059512",
      "PI Name": "TADROSS, MICHAEL R",
      "PI Email": "michael.tadross@duke.edu",
      "Human / Animal": "Animal",
      "Species (Regular Name)": "Mouse",
      "Species (Scientific Name)": "Mus musculus",
      "Sensors": "3D Pose estimation cameras",
      "Data Modalities": "Video data",
      "Approaches": "Deep neural network, 3D pose estimation",
      "Sensors (Standardized)": "video cameras",
      "Sensors\n(Dropdown)": "video cameras",
      "Data Modalities (Standardized)": "visual; kinematic",
      "Data Modalities \n(Dropdown)": "visual, kinematic",
      "Approaches (Standardized)": "pose estimation; behavior tracking",
      "Approaches \n(Dropdown)": "pose estimation, behavior tracking",
      "Data Generation Start": "4/12/21",
      "Proposal objective": "\"The aim of this proposal is to develop an innovative new system, including hardware assemblies and machine learning algorithms, for continuous, high-resolution 3D quantification of behavioral and eliciting stimulus dynamics in a natural mouse prey capture paradigm.\"",
      "Build hardware?": "Yes",
      "Hardware type": "Arena",
      "Hardware details": "Build a scalable recording environment, allowing multiple organisms during cricket hunting",
      "Build software?": "Yes",
      "Software type": "Analysis, Behavioral quantification, ML model, End product, Tracking, Pose estimation",
      "Software details": "convolutional NN processing multi-perspective video; 3D tracking algorithm to support out-of-the-box generalization to cloned setups; analysis to isolate kinematic and action variables",
      "DMSP software": "open-source python; parse behavior into modules; behavior tracking system; deep network for 3D kinematic tracking",
      "Collect data?": "Yes",
      "Data type": "Behavioral, Video, Visual, Stimulation, Kinematic",
      "Data details": "Pose tracking, multiple animals; ecological recording; healthy vs parkinsons disease or retinal degeneration animals; ",
      "DMSP data": "high-resolution behavior; hundreds of hours of video and 3D kinematic traces of interacting rats; social behavior annotations for all frames of video",
      "Challenges (kickoff videos)": "1.\t3D behavior reconstruction/pose-tracking from multi-view cameras (for single as well as multiple rodents)",
      "Proposed WGs": "Deep learning",
      "Which types of data are you collecting for BBQS?": "Behavioral data (e.g., video recordings, motion capture, eye tracking, gait analysis);Social interaction data (e.g., proximity sensors, communication logs)",
      "Which types of data are you collecting for BBQS?\n(Dropdown)": "Behavioral data, Social interaction data",
      "What types of behavior are you interested in?": "Locomotion and Movement (for example: walking, running, swimming, climbing, jumping);Gait Analysis (for example: stride length, step frequency, limb coordination);Posture and Balance (for example: standing, sitting, lying down, postural sway);Social Interactions (for example: grooming, play behavior, aggression, mating behavior);Exploratory Behavior (for example: novel object interaction, maze navigation, environmental exploration)\t\t",
      "What types of behavior are you interested in? \n(high level)": "Locomotion and Movement, Gait Analysis, Posture and Balance, Social Interactions, Exploratory Behavior\t\t",
      "What types of behavior are you interested in? \n(high level; dropdown)": "Locomotion and Movement, Gait Analysis, Posture and Balance, Social Interactions, Exploratory Behavior\t\t",
      "What statistical or computational methods do you primarily rely on for your data analysis?": "Classical statistical methods (e.g., t-tests, ANOVA);Time series analysis;Unsupervised machine learning (e.g., clustering, PCA);Supervised machine learning (e.g., classification, regression);Deep learning (e.g., neural networks, CNNs, RNNs)\t",
      "What types of analyses are you performing on your neural and behavioral data?": "Statistical analysis, Signal processing, Dimensionality reduction, Dynamical systems modeling, Time-frequency analysis, Bayesian inference, Encoding models, Decoding models, Correlation analysis, Regression models        "
    },
    {
      "award number": "R34DA059716",
      "Type": "R34",
      "Link": "RFA-DA-23-030",
      "#": "DA23-030",
      "PoP years (max)": "2",
      "Title": "Interpersonal behavioral synchrony in virtual and in-person dyadic conversation",
      "Award #": "R34DA059716",
      "PI Name": "PARVAZ, MUHAMMAD ADEEL",
      "PI Email": "muhammad.parvaz@mssm.edu",
      "Human / Animal": "Human",
      "Species (Regular Name)": "Human",
      "Species (Scientific Name)": "Homo sapiens",
      "Sensors": "Tobii Glasses, plethysmograph, EEG (hyperscanning), Heart rate sensors",
      "Data Modalities": "De-identified time-stamped transcripts, PRAAT/OpenSmile audio files, Video files for frame-by-frame analysis, EEG hyperscanning",
      "Approaches": "Facial action units analysis, Deep neural network emotion labels (HUME-AI), Electrodermal activity, Eye-tracking, Monitoring peripheral capillary oxygen saturation",
      "Sensors (Standardized)": "eye tracker; EEG; EDA; plethsymography; heart rate sensors",
      "Sensors\n(Dropdown)": "eye tracker, EEG, EDA, plethsymography, heart rate sensors",
      "Data Modalities (Standardized)": "audio; visual; neural",
      "Data Modalities \n(Dropdown)": "audio, visual, neural",
      "Approaches (Standardized)": "facial expression analysis; EEG hyperscanning; emotion estimation; feature extraction; correlation analysis",
      "Approaches \n(Dropdown)": "facial expression analysis, EEG hyperscanning, emotion estimation, feature extraction, correlation analysis",
      "Data Generation Start": "1/1/25",
      "Proposal objective": "\"In this planning grant we convene a diverse multidisciplinary team, including ethicists, to develop and validate a tool for quantification of dyadic communication behavior across modalities, sensitive to temporal patterns, and computational frameworks for analysis. The tool will be both in-person and via remote platform.\"",
      "Build hardware?": "Yes",
      "Hardware type": "Testing room",
      "Hardware details": "Testing rooms for multimodal recording of dyatic communication; ",
      "Build software?": "Yes",
      "Software type": "Analysis, Behavioral quantification, Temporal data",
      "Software details": "A tool for quantification of multi-modal dyadic communication sensitive to temporal patterns",
      "DMSP software": "intructions and parameter choices will be included for GUI-based analyses",
      "Collect data?": "Yes",
      "Data type": "Multi-modal, Remote collection, Human, High temporal resolution, Audio,  Neural, Video, Visual, Non-invasive",
      "Data details": "Dyadic communication, including face expression and gaze, acoustics (prosody and pauses) turning taking, gestures and language.\nEEG hyperscanning",
      "DMSP data": "demographic, questionnaire, behavioral (speech, video, oculomotor); EEG data.",
      "Challenges (kickoff videos)": "1.\tMultimodal recording (audio and video)\n2.\tEye-tracking and EEG scanning (pilot)",
      "Proposed WGs": "Deep learning",
      "Which types of data are you collecting for BBQS?": "Neural data (e.g., EEG, MEG, fMRI, ECoG, single-unit recordings);Behavioral data (e.g., video recordings, motion capture, eye tracking, gait analysis);Physiological data (e.g., heart rate, skin conductance, respiratory rate);Cognitive performance data (e.g., reaction times, accuracy, task performance metrics);Social interaction data (e.g., proximity sensors, communication logs);Self-report data (e.g., questionnaires, diaries, surveys);Wearable sensor data (e.g., accelerometers, gyroscopes, smartwatches)\t",
      "Which types of data are you collecting for BBQS?\n(Dropdown)": "Neural data, Behavioral data, Physiological data, Cognitive performance data, Social interaction data, Self-report data, Wearable sensor data",
      "What types of behavior are you interested in?": "Posture and Balance (for example: standing, sitting, lying down, postural sway);Eye Movements (for example: saccades, fixations, smooth pursuit);Facial Expressions (for example: smiling, frowning, grimacing);Vocalizations and Speech (for example: talking, crying, barking, chirping);Social Interactions (for example: grooming, play behavior, aggression, mating behavior);Emotional and Stress Responses (for example: anxiety-related behaviors, fear responses, stress-induced behaviors)",
      "What types of behavior are you interested in? \n(high level)": "Posture and Balance, Eye Movements, Facial Expressions, Vocalizations and Speech, Social Interactions, Emotional and Stress Responses",
      "What types of behavior are you interested in? \n(high level; dropdown)": "Posture and Balance, Eye Movements, Facial Expressions, Vocalizations and Speech, Social Interactions, Emotional and Stress Responses",
      "What statistical or computational methods do you primarily rely on for your data analysis?": "Classical statistical methods (e.g., t-tests, ANOVA);Time series analysis;Unsupervised machine learning (e.g., clustering, PCA);Supervised machine learning (e.g., classification, regression);Deep learning (e.g., neural networks, CNNs, RNNs)\t",
      "What types of analyses are you performing on your neural and behavioral data?": "Statistical analysis, Signal processing, Dynamical systems modeling, Time-frequency analysis, Network analysis, Correlation analysis, Regression models\t"
    },
    {
      "award number": "R34DA059723",
      "Type": "R34",
      "Link": "RFA-DA-23-030",
      "#": "DA23-030",
      "PoP years (max)": "2",
      "Title": "Multimodal behavioral analysis of oromanual food-handling in freely moving animals",
      "Award #": "R34DA059723",
      "PI Name": "SHEPHERD, GORDON M",
      "PI Email": "g-shepherd@northwestern.edu",
      "Human / Animal": "Animal",
      "Species (Regular Name)": "Mouse (and other unspecified rodents)",
      "Species (Scientific Name)": "Mus musculus (and other unspecified rodents)",
      "Sensors": "Intranasal Thermistor, Electromyography, Acoustic microphones",
      "Data Modalities": "Mainly high-speed video",
      "Approaches": "Multimodal analysis of food-handling behavior in freely moving mice and other animals, DeepLabCut pose estimation",
      "Sensors (Standardized)": "intranasal thermistor; EMG; microphones",
      "Sensors\n(Dropdown)": "intranasal thermistor, EMG, microphones",
      "Data Modalities (Standardized)": "visual; kinematic; physiological",
      "Data Modalities \n(Dropdown)": "visual, kinematic, physiological",
      "Approaches (Standardized)": "physiological signal analysis; pose estimation; EMG tracking",
      "Approaches \n(Dropdown)": "physiological signal analysis, pose estimation, EMG tracking",
      "Data Generation Start": "2/1/24",
      "Proposal objective": "\"The overall objective is to develop new experimental and analytical paradigms for recording food-handling behavior with high spatiotemporal resolution in freely moving animals, with a focus on understanding how elemental sub- movements are assembled into distinct goal-directed actions coordinated across multiple body parts.\"",
      "Build hardware?": "Yes",
      "Hardware type": "Recording device",
      "Hardware details": "A videographic recording arena + robotic camera positioning system",
      "Build software?": "Yes",
      "Software type": "ML model, Tracking, Behavioral quantification, Analysis",
      "Software details": "Ethogramming, ML-based tracking, modeling, quantitative analysis of food-handling",
      "DMSP software": "use DeepLabCut for tracking",
      "Collect data?": "Yes",
      "Data type": "Physiological, Behavioral",
      "Data details": "electromyography from jaw-controller and forelimb muscles, breathing",
      "DMSP data": "physiological (electromyography, breathing/olfactory) recordings and DeepLabCut tracking data and models",
      "Challenges (kickoff videos)": "1.\tIdentification of specific movements from high-speed camera recordings of oromanual food handling\n2.\tRecording/identification of activity from EMGs, breathing, and acoustic signals.",
      "Proposed WGs": "Deep Learning",
      "Which types of data are you collecting for BBQS?": "Behavioral data (e.g., video recordings, motion capture, eye tracking, gait analysis);Physiological data (e.g., heart rate, skin conductance, respiratory rate)",
      "Which types of data are you collecting for BBQS?\n(Dropdown)": "Behavioral data, Physiological data",
      "What types of behavior are you interested in?": "Locomotion and Movement (for example: walking, running, swimming, climbing, jumping);Feeding and Drinking (for example: eating patterns, drinking frequency, food preference);Exploratory Behavior (for example: novel object interaction, maze navigation, environmental exploration)\t",
      "What types of behavior are you interested in? \n(high level)": "Locomotion and Movement, Feeding and Drinking, Exploratory Behavior\t",
      "What types of behavior are you interested in? \n(high level; dropdown)": "Locomotion and Movement, Feeding and Drinking, Exploratory Behavior\t",
      "What statistical or computational methods do you primarily rely on for your data analysis?": "Classical statistical methods (e.g., t-tests, ANOVA);Time series analysis;Unsupervised machine learning (e.g., clustering, PCA);Supervised machine learning (e.g., classification, regression);Deep learning (e.g., neural networks, CNNs, RNNs)\t",
      "What types of analyses are you performing on your neural and behavioral data?": "Statistical analysis, Signal processing, Time-frequency analysis, Correlation analysis, Regression models        "
    },
    {
      "award number": "R34DA059514",
      "Type": "R34",
      "Link": "RFA-DA-23-030",
      "#": "DA23-030",
      "PoP years (max)": "2",
      "Title": "Towards High-Resolution Neuro-Behavioral Quantification of Sheep in the Field to Study Complex Social Behaviors",
      "Award #": "R34DA059514",
      "PI Name": "KEMERE, CALEB",
      "PI Email": "caleb.kemere@rice.edu",
      "Human / Animal": "Animal",
      "Species (Regular Name)": "Sheep",
      "Species (Scientific Name)": "Ovis aries",
      "Sensors": "First-person-view cameras",
      "Data Modalities": "Sound",
      "Approaches": "Quantification of flocking sheep behavior",
      "Sensors (Standardized)": "video cameras; microphones; wireless neural; GPS",
      "Sensors\n(Dropdown)": "video cameras, microphones, wireless neural, GPS",
      "Data Modalities (Standardized)": "visual; audio; neural; spatial",
      "Data Modalities \n(Dropdown)": "visual, audio, neural, spatial",
      "Approaches (Standardized)": "speech analysis; behavior tracking",
      "Approaches \n(Dropdown)": "speech analysis, behavior tracking",
      "Data Generation Start": "3/1/25",
      "Proposal objective": "\"This proposal seeks to lay the groundwork for conducting large scale neural recording experiments in herds of sheep engaging in their normal individual and collective behaviors in the field.\"",
      "Build hardware?": "Yes",
      "Hardware type": "Recording device",
      "Hardware details": "Head-mounted device to sense visual sensorium, neural recording (completely implanted, wirelessly powered)",
      "Build software?": "Yes",
      "Software type": "Tracking, Analysis, Prediction, Temporal data",
      "Software details": "Embedded systems for high spatio-temporal resolution of tracking behavior, measure visual sensorium, individual behavioral prediction based on internal state dynamics and environment, motivated models of behavior",
      "DMSP software": "Python for processing/analysis",
      "Collect data?": "Yes",
      "Data type": "Animal, Neural , Environmental, Social interaction, Visual, Wearable sensor, Audio, Video, Behavioral",
      "Data details": "Measurement of individual herd members, visual sensorium, neural recording, GPS ",
      "DMSP data": "Raw electrophysiological data (SpikeGadgets or OpenEhys); behavior tracking with video data; other digital and behavior data includes timestamps of ripple disruption simulation (GPIO pulses). Perfused brain tissue (microscopy).",
      "Challenges (kickoff videos)": "1.\tDevelopment of instrumentation for implanted neural recordings of sheep    (major)\n2.\tInstrumentation/collection of audio, video, and GPS data",
      "Proposed WGs": "Instrumentation, Deep Learning",
      "Which types of data are you collecting for BBQS?": "Neural data (e.g., EEG, MEG, fMRI, ECoG, single-unit recordings);Behavioral data (e.g., video recordings, motion capture, eye tracking, gait analysis);Environmental data (e.g., ambient light, temperature, sound levels);Social interaction data (e.g., proximity sensors, communication logs);Wearable sensor data (e.g., accelerometers, gyroscopes, smartwatches)\t",
      "Which types of data are you collecting for BBQS?\n(Dropdown)": "Neural data, Behavioral data, Environmental data, Social interaction data, Wearable sensor data",
      "What types of behavior are you interested in?": "Locomotion and Movement (for example: walking, running, swimming, climbing, jumping);Gait Analysis (for example: stride length, step frequency, limb coordination);Posture and Balance (for example: standing, sitting, lying down, postural sway);Eye Movements (for example: saccades, fixations, smooth pursuit);Vocalizations and Speech (for example: talking, crying, barking, chirping);Social Interactions (for example: grooming, play behavior, aggression, mating behavior);Feeding and Drinking (for example: eating patterns, drinking frequency, food preference);Exploratory Behavior (for example: novel object interaction, maze navigation, environmental exploration)",
      "What types of behavior are you interested in? \n(high level)": "Locomotion and Movement, Gait Analysis, Posture and Balance, Eye Movements, Vocalizations and Speech, Social Interactions, Feeding and Drinking, Exploratory Behavior",
      "What types of behavior are you interested in? \n(high level; dropdown)": "Locomotion and Movement, Gait Analysis, Posture and Balance, Eye Movements, Vocalizations and Speech, Social Interactions, Feeding and Drinking, Exploratory Behavior",
      "What statistical or computational methods do you primarily rely on for your data analysis?": "Classical statistical methods (e.g., t-tests, ANOVA);Time series analysis;Unsupervised machine learning (e.g., clustering, PCA);Supervised machine learning (e.g., classification, regression);Deep learning (e.g., neural networks, CNNs, RNNs)\t",
      "What types of analyses are you performing on your neural and behavioral data?": "Statistical analysis, Signal processing, Dimensionality reduction, Dynamical systems modeling, Time-frequency analysis\t"
    },
    {
      "award number": "R34DA059500",
      "Type": "R34",
      "Link": "RFA-DA-23-030",
      "#": "DA23-030",
      "PoP years (max)": "2",
      "Title": "Transformative Optical Imaging of Brain & Behavior in Navigating Genetic Species",
      "Award #": "R34DA059500",
      "PI Name": "WANG, JANE",
      "PI Email": "zw24@cornell.edu",
      "Human / Animal": "Animal",
      "Species (Regular Name)": "Zebrafish, Fruit fly, Crustacean",
      "Species (Scientific Name)": "Danio rerio, Drosophila melanogaster, Parhyale hawaiensis",
      "Sensors": "Behavioral apparatus, Bioluminescent indicators",
      "Data Modalities": "Visual, Mechanosensory, Olfactory, Physiological",
      "Approaches": "Quantitative behavior analysis, Real-time tracking, Physical modeling, High-resolution body posture analysis, Limb kinematics analysis, Imaging of neural activity, Quantification of stimulus-guided navigation",
      "Sensors (Standardized)": "video cameras; neuroimaging sensors",
      "Sensors\n(Dropdown)": "video cameras, neuroimaging sensors",
      "Data Modalities (Standardized)": "visual; kinematic; neural",
      "Data Modalities \n(Dropdown)": "visual, kinematic, neural",
      "Approaches (Standardized)": "pose estimation; behavior tracking; neural activity modeling",
      "Approaches \n(Dropdown)": "pose estimation, behavior tracking, neural activity modeling",
      "Data Generation Start": "12/1/24",
      "Proposal objective": "\"understand how patterns of neural activity allow for effective navigation of complex and diverse environments\"",
      "Build hardware?": "Yes",
      "Hardware type": "Arena, Recording device",
      "Hardware details": "Laminar flow chamber (under water), behavioral apparatus",
      "Build software?": "Yes",
      "Software type": "Tracking, Behavioral quantification",
      "Software details": "real-time behavioral tracking at high resolution; physical modeling",
      "DMSP software": "Python for processing/analysis",
      "Collect data?": "Yes",
      "Data type": "Neural , Behavioral, Non-invasive",
      "Data details": "Unconstrained behavior, non-invasive neural activity, freely moving animals, mechanosensory flow, odor, and visual stimuli; bioluminescence-based",
      "DMSP data": "Raw electrophysiological data (SpikeGadgets or OpenEhys); behavior tracking with video data; other digital and behavior data includes timestamps of ripple disruption simulation (GPIO pulses). Perfused brain tissue (microscopy).",
      "Challenges (kickoff videos)": "1.\tNon-invasive neuronal activity in freely moving animals\n2.\tVideo recordings of movements as well as bioluminescent movement recoding in fish (this can be considered biochemical)\n3.\tFor the bioluminescence signal, seeking a comparable temporal resolution as calcium imaging but that will be too slow to resolve LFP.",
      "Proposed WGs": "Instrumentation, Deep Learning",
      "Which types of data are you collecting for BBQS?": "Neural data (e.g., EEG, MEG, fMRI, ECoG, single-unit recordings);Behavioral data (e.g., video recordings, motion capture, eye tracking, gait analysis);Environmental data (e.g., ambient light, temperature, sound levels);Social interaction data (e.g., proximity sensors, communication logs);Wearable sensor data (e.g., accelerometers, gyroscopes, smartwatches)\t",
      "Which types of data are you collecting for BBQS?\n(Dropdown)": "Neural data, Behavioral data, Environmental data, Social interaction data, Wearable sensor data",
      "What types of behavior are you interested in?": "Locomotion and Movement (for example: walking, running, swimming, climbing, jumping);Gait Analysis (for example: stride length, step frequency, limb coordination);Posture and Balance (for example: standing, sitting, lying down, postural sway);Eye Movements (for example: saccades, fixations, smooth pursuit);Vocalizations and Speech (for example: talking, crying, barking, chirping);Social Interactions (for example: grooming, play behavior, aggression, mating behavior);Feeding and Drinking (for example: eating patterns, drinking frequency, food preference);Exploratory Behavior (for example: novel object interaction, maze navigation, environmental exploration)",
      "What types of behavior are you interested in? \n(high level)": "Locomotion and Movement, Gait Analysis, Posture and Balance, Eye Movements, Vocalizations and Speech, Social Interactions, Feeding and Drinking, Exploratory Behavior",
      "What types of behavior are you interested in? \n(high level; dropdown)": "Locomotion and Movement, Gait Analysis, Posture and Balance, Eye Movements, Vocalizations and Speech, Social Interactions, Feeding and Drinking, Exploratory Behavior",
      "What statistical or computational methods do you primarily rely on for your data analysis?": "Classical statistical methods (e.g., t-tests, ANOVA);Time series analysis;Unsupervised machine learning (e.g., clustering, PCA);Supervised machine learning (e.g., classification, regression);Deep learning (e.g., neural networks, CNNs, RNNs)\t",
      "What types of analyses are you performing on your neural and behavioral data?": "Statistical analysis, Signal processing, Dimensionality reduction, Dynamical systems modeling, Time-frequency analysis\t"
    },
    {
      "award number": "R34DA061984",
      "Type": "R34",
      "Link": "RFA-DA-23-030",
      "#": "DA23-030",
      "PoP years (max)": "2",
      "Title": "Quantifying organism-environment interactions in a new model system for neuroscience",
      "Award #": "R34DA061984",
      "PI Name": "SRIVASTAVA, MANSI ",
      "PI Email": "mansi@oeb.harvard.edu",
      "Human / Animal": "Animal",
      "Species (Regular Name)": "Aacoel worm",
      "Species (Scientific Name)": "Hofstenia miamia",
      "Sensors": "Behavioral apparatus, Bioluminescent indicators",
      "Data Modalities": "Visual, Mechanosensory, Olfactory, Physiological",
      "Approaches": "Quantitative behavior analysis, Real-time tracking, Physical modeling, High-resolution body posture analysis, Limb kinematics analysis, Imaging of neural activity, Quantification of stimulus-guided navigation",
      "Sensors (Standardized)": "video cameras; neuroimaging sensors",
      "Sensors\n(Dropdown)": "video cameras, neuroimaging sensors",
      "Data Modalities (Standardized)": "visual; kinematic; neural",
      "Data Modalities \n(Dropdown)": "visual, kinematic, neural",
      "Approaches (Standardized)": "pose estimation; behavior tracking; neural activity modeling",
      "Approaches \n(Dropdown)": "pose estimation, behavior tracking, neural activity modeling",
      "Data Generation Start": "12/1/24",
      "Proposal objective": "\"This project aims to develop a new animal model for neuroscience that allows synchronous measurement of behavior and of the environment at the level of the whole organism.\"",
      "Build hardware?": "Yes",
      "Hardware type": "Arena, Recording device",
      "Hardware details": "Environment (water flow) methods?",
      "Build software?": "Yes",
      "Software type": "Tracking, Synchronization, Behavioral quantification, Pose estimation",
      "Software details": "measure/quantify worm behavior and environment; pose estimation to infer worm action sequences; tracking",
      "DMSP software": "use DeepLabCut and SLEAP for pose estimation; cellpose to track animals; to develop pipelines to analyse tracked postural data",
      "Collect data?": "Yes",
      "Data type": "Neural , Environmental, Behavioral, Video, Genetic",
      "Data details": "Environment; pose; water flow; neural activity",
      "DMSP data": "behavioral video data; imaging data (neural activity/structure)",
      "Challenges (kickoff videos)": "1.\tNon-invasive neuronal activity in freely moving animals\n2.\tVideo recordings of movements as well as bioluminescent movement recoding in fish (this can be considered biochemical)\n3.\tFor the bioluminescence signal, seeking a comparable temporal resolution as calcium imaging but that will be too slow to resolve LFP.",
      "Proposed WGs": "Instrumentation, Deep Learning",
      "Which types of data are you collecting for BBQS?": "Neural data (e.g., EEG, MEG, fMRI, ECoG, single-unit recordings);Behavioral data (e.g., video recordings, motion capture, eye tracking, gait analysis);Environmental data (e.g., ambient light, temperature, sound levels);Social interaction data (e.g., proximity sensors, communication logs);Wearable sensor data (e.g., accelerometers, gyroscopes, smartwatches)\t",
      "Which types of data are you collecting for BBQS?\n(Dropdown)": "Neural data, Behavioral data, Environmental data, Social interaction data, Wearable sensor data",
      "What types of behavior are you interested in?": "Locomotion and Movement (for example: walking, running, swimming, climbing, jumping);Gait Analysis (for example: stride length, step frequency, limb coordination);Posture and Balance (for example: standing, sitting, lying down, postural sway);Eye Movements (for example: saccades, fixations, smooth pursuit);Vocalizations and Speech (for example: talking, crying, barking, chirping);Social Interactions (for example: grooming, play behavior, aggression, mating behavior);Feeding and Drinking (for example: eating patterns, drinking frequency, food preference);Exploratory Behavior (for example: novel object interaction, maze navigation, environmental exploration)",
      "What types of behavior are you interested in? \n(high level)": "Locomotion and Movement, Gait Analysis, Posture and Balance, Eye Movements, Vocalizations and Speech, Social Interactions, Feeding and Drinking, Exploratory Behavior",
      "What types of behavior are you interested in? \n(high level; dropdown)": "Locomotion and Movement, Gait Analysis, Posture and Balance, Eye Movements, Vocalizations and Speech, Social Interactions, Feeding and Drinking, Exploratory Behavior",
      "What statistical or computational methods do you primarily rely on for your data analysis?": "Classical statistical methods (e.g., t-tests, ANOVA);Time series analysis;Unsupervised machine learning (e.g., clustering, PCA);Supervised machine learning (e.g., classification, regression);Deep learning (e.g., neural networks, CNNs, RNNs)\t",
      "What types of analyses are you performing on your neural and behavioral data?": "Statistical analysis, Signal processing, Dimensionality reduction, Dynamical systems modeling, Time-frequency analysis\t"
    },
    {
      "award number": "R34DA061924",
      "Type": "R34",
      "Link": "RFA-DA-23-030",
      "#": "DA23-030",
      "PoP years (max)": "2",
      "Title": "Mapping dynamic transitions across neural, behavioral, and social scales in interacting animals",
      "Award #": "R34DA061924",
      "PI Name": "FROHLICH, FLAVIO",
      "PI Email": "mengsen@msu.edu",
      "Human / Animal": "Animal",
      "Species (Regular Name)": "Ferret",
      "Species (Scientific Name)": "Hofstenia miamia",
      "Sensors": "Behavioral apparatus, Bioluminescent indicators",
      "Data Modalities": "Visual, Mechanosensory, Olfactory, Physiological",
      "Approaches": "Quantitative behavior analysis, Real-time tracking, Physical modeling, High-resolution body posture analysis, Limb kinematics analysis, Imaging of neural activity, Quantification of stimulus-guided navigation",
      "Sensors (Standardized)": "video cameras; neuroimaging sensors",
      "Sensors\n(Dropdown)": "video cameras, neuroimaging sensors",
      "Data Modalities (Standardized)": "visual; kinematic; neural",
      "Data Modalities \n(Dropdown)": "visual, kinematic, neural",
      "Approaches (Standardized)": "pose estimation; behavior tracking; neural activity modeling",
      "Approaches \n(Dropdown)": "pose estimation, behavior tracking, neural activity modeling",
      "Data Generation Start": "12/1/24",
      "Proposal objective": "\"The main objective of this project is to develop a computational-experimental framework to construct multiscale models of naturalistic social interaction connecting the spiking dynamics of neurons, brain oscillations, body movements, and macroscopic behavioral states\"",
      "Build hardware?": "Yes",
      "Hardware type": "Arena, Recording device",
      "Hardware details": "Environment (water flow) methods?",
      "Build software?": "Yes",
      "Software type": "Analysis, Behavioral quantification, Temporal data, Computer vision, ML model, Tracking",
      "Software details": "topological time series analysis; computer vision for motion tracking, machine learning for cross-scale mapping of state transitions; annotations",
      "DMSP software": "motion tracking data computed via DeepLabCut.\ntransition networks; cross-scale mappings",
      "Collect data?": "Yes",
      "Data type": "Neural , Animal, Social interaction, Behavioral, \"Invasive (e.g. single unit, ECoG)\", Video, Annotation, Wearable sensor",
      "Data details": "behavioral and electrophysiology; body movements; social interaction; brain oscillations",
      "DMSP data": "primary raw data: ferret electrophysiology; videos of social interaction. annotated behavioral states; motion tracking data computed from videos and accelerometer data.",
      "Challenges (kickoff videos)": "1.\tNon-invasive neuronal activity in freely moving animals\n2.\tVideo recordings of movements as well as bioluminescent movement recoding in fish (this can be considered biochemical)\n3.\tFor the bioluminescence signal, seeking a comparable temporal resolution as calcium imaging but that will be too slow to resolve LFP.",
      "Proposed WGs": "Instrumentation, Deep Learning",
      "Which types of data are you collecting for BBQS?": "Neural data (e.g., EEG, MEG, fMRI, ECoG, single-unit recordings);Behavioral data (e.g., video recordings, motion capture, eye tracking, gait analysis);Social interaction data (e.g., proximity sensors, communication logs);Wearable sensor data (e.g., accelerometers, gyroscopes, smartwatches)\t",
      "Which types of data are you collecting for BBQS?\n(Dropdown)": "Neural data, Behavioral data, Social interaction data, Wearable sensor data",
      "What types of behavior are you interested in?": "Locomotion and Movement (for example: walking, running, swimming, climbing, jumping);Social Interactions (for example: grooming, play behavior, aggression, mating behavior)\t",
      "What types of behavior are you interested in? \n(high level)": "Locomotion and Movement, Social Interactions\t",
      "What types of behavior are you interested in? \n(high level; dropdown)": "Locomotion and Movement, Social Interactions\t",
      "What statistical or computational methods do you primarily rely on for your data analysis?": "Classical statistical methods (e.g., t-tests, ANOVA);Time series analysis;Unsupervised machine learning (e.g., clustering, PCA);Supervised machine learning (e.g., classification, regression);Deep learning (e.g., neural networks, CNNs, RNNs)\t",
      "What types of analyses are you performing on your neural and behavioral data?": "Statistical analysis, Signal processing, Dimensionality reduction, Dynamical systems modeling, Time-frequency analysis, Network analysis        "
    },
    {
      "award number": "R61MH135106",
      "Type": "R61 / R33",
      "Link": "RFA-MH-22-240",
      "#": "MH22-240",
      "PoP years (max)": "5",
      "Title": "Synchronized neuronal and peripheral biomarker recordings in freely moving humans",
      "Award #": "R61MH135106",
      "PI Name": "SUTHANA, NANTHIA A",
      "PI Email": "nsuthana@mednet.ucla.edu",
      "Human / Animal": "Human",
      "Species (Regular Name)": "Human",
      "Species (Scientific Name)": "Homo sapiens",
      "Sensors": "Wearable Biophysical sensors, iEEG, Peripheral biochemical sensors",
      "Data Modalities": "Intracranial neural recordings, Psychological stress, Peripheral biochemical sensing",
      "Approaches": "Validation of neural and peripheral biomarker recordings and approach-avoidance behavior (Trier Social Stress Test)",
      "Sensors (Standardized)": "iEEG; motion tracking; eye tracker; cortisol wearable; epinephrine wearable",
      "Sensors\n(Dropdown)": "iEEG, motion tracking, eye tracker, cortisol wearable, epinephrine wearable",
      "Data Modalities (Standardized)": "neural; physiological; kinematic; biochemical",
      "Data Modalities \n(Dropdown)": "neural, physiological, kinematic, biochemical",
      "Approaches (Standardized)": "stress analysis; approach-avoidance modeling; augmented reality; virtual reality",
      "Approaches \n(Dropdown)": "stress analysis, approach-avoidance modeling, augmented reality, virtual reality",
      "Data Generation Start": "11/1/24",
      "Proposal objective": "\"The proposed project will develop a novel platform that enables simultaneous single- neuron or iEEG, biochemical (cortisol, epinephrine), and biophysical (heart rate, skin conductance, and body and eye movements) activity to be recorded in freely moving human participants.\"",
      "Build hardware?": "Yes",
      "Hardware type": "Recording device",
      "Hardware details": "platform to intracranially record single-neuron/LFP activity in humans; biophysical and biochemical wearables - synchronised",
      "Build software?": "Yes",
      "Software type": "Analysis, VR/AR",
      "Software details": "Software for platform; neural data analysis; VR/AR for experiments?",
      "DMSP software": "analysis; open source",
      "Collect data?": "Yes",
      "Data type": "Wearable sensor, Neural , \"Invasive (e.g. single unit, ECoG)\", Biochemical, Multi-modal, Eye-tracking, Human, Behavioral, Video",
      "Data details": "iEEG/single neuron, peripheral monitoring, walking speed, cortisol & epinepherine from sweat",
      "DMSP data": "demographic, medical history, neuroimaging, electrophysiological, neuropsychological, behavioral task, biochemical (CORT and EPI levels), physiological  (HRV, SCR, EDA). motion capture, eye tracking, monitoring cameras, task-related variables. ",
      "Challenges (kickoff videos)": "1.        Optimize the wearability of the integrated sensor system (currently sitting in a bag)\n2.        Integration of a single-neuron activity monitoring system (with the wearable bag)\n3.        Development of biochemical smartwatches to sample cortisol and epinephrine in sweat and subsequent integration with the wearable device.\n4.        Investigate neural mechanisms of naturalistic behaviors in freely-moving humans (later)",
      "Proposed WGs": "Instrumentation, Data, Ethics",
      "Which types of data are you collecting for BBQS?": "Neural data (e.g., EEG, MEG, fMRI, ECoG, single-unit recordings);Behavioral data (e.g., video recordings, motion capture, eye tracking, gait analysis);Physiological data (e.g., heart rate, skin conductance, respiratory rate);Cognitive performance data (e.g., reaction times, accuracy, task performance metrics);Biochemical data (e.g., hormone levels, neurotransmitter concentrations);Social interaction data (e.g., proximity sensors, communication logs);Self-report data (e.g., questionnaires, diaries, surveys);Wearable sensor data (e.g., accelerometers, gyroscopes, smartwatches)",
      "Which types of data are you collecting for BBQS?\n(Dropdown)": "Neural data, Behavioral data, Physiological data, Cognitive performance data, Biochemical data, Social interaction data, Self-report data, Wearable sensor data",
      "What types of behavior are you interested in?": "Locomotion and Movement (for example: walking, running, swimming, climbing, jumping);Eye Movements (for example: saccades, fixations, smooth pursuit);Cognitive and Memory Tasks (for example: problem-solving, decision-making, memory recall);Social Interactions (for example: grooming, play behavior, aggression, mating behavior);Exploratory Behavior (for example: novel object interaction, maze navigation, environmental exploration);Emotional and Stress Responses (for example: anxiety-related behaviors, fear responses, stress-induced behaviors);Sleep and Rest (for example: sleep duration, sleep cycles, napping behavior);Operant Conditioning and Learning (for example: lever pressing, maze learning, reward-based tasks)\t",
      "What types of behavior are you interested in? \n(high level)": "Locomotion and Movement, Eye Movements, Cognitive and Memory Tasks, Social Interactions, Exploratory Behavior, Emotional and Stress Responses, Sleep and Rest, Operant Conditioning and Learning\t",
      "What types of behavior are you interested in? \n(high level; dropdown)": "Locomotion and Movement, Eye Movements, Cognitive and Memory Tasks, Social Interactions, Exploratory Behavior, Emotional and Stress Responses, Sleep and Rest, Operant Conditioning and Learning\t",
      "What statistical or computational methods do you primarily rely on for your data analysis?": "Classical statistical methods (e.g., t-tests, ANOVA);Time series analysis;Unsupervised machine learning (e.g., clustering, PCA);Supervised machine learning (e.g., classification, regression);Deep learning (e.g., neural networks, CNNs, RNNs)\t",
      "What types of analyses are you performing on your neural and behavioral data?": "Statistical analysis, Signal processing, Dimensionality reduction, Time-frequency analysis, Decoding models, Correlation analysis, Regression models"
    },
    {
      "award number": "R61MH135109",
      "Type": "R61 / R33",
      "Link": "RFA-MH-22-240",
      "#": "MH22-240",
      "PoP years (max)": "5",
      "Title": "Capturing Autobiographical memory formation in People moving Through real-world spaces Using synchronized wearables and intracranial Recordings of EEG",
      "Award #": "R61MH135109",
      "PI Name": "INMAN, CORY SHIELDS",
      "PI Email": "cory.inman@psych.utah.edu",
      "Human / Animal": "Human",
      "Species (Regular Name)": "Human",
      "Species (Scientific Name)": "Homo sapiens",
      "Sensors": "Eye Tracking, Smartphone sensors (GPS, Accelerometry), Neuropace RNS system",
      "Data Modalities": "Wearable eye-tracking data, High-res chest-mounted video, Brain data from Neuropace RNS system, Smartwatch physiology data",
      "Approaches": "Sensor data upload to Neuropace",
      "Sensors (Standardized)": "iEEG; accelerometer; IMU; video cameras; GPS; eye tracker",
      "Sensors\n(Dropdown)": "iEEG, accelerometer, IMU, video cameras, GPS, eye tracker",
      "Data Modalities (Standardized)": "neural; kinematic; visual; spatial",
      "Data Modalities \n(Dropdown)": "neural, kinematic, visual, spatial",
      "Approaches (Standardized)": "spatial navigation tracking; augmented reality",
      "Approaches \n(Dropdown)": "spatial navigation tracking, augmented reality",
      "Data Generation Start": "6/4/25",
      "Proposal objective": "\"the proposed research project aims to develop a smartphone-based recording application (CAPTURE app; R61 phase) synchronized with wearables and invasive neural recordings during real-world behaviors like autobiographical memory encoding (R33 phase).\"",
      "Build hardware?": "Yes",
      "Hardware type": "Recording device",
      "Hardware details": "wearable app/tool to capture real-world cognitive processes; RNS wand tool, neuropace wand; smartwatch; eye-tracking",
      "Build software?": "Yes",
      "Software type": "Encoding, Analysis, Synchronization",
      "Software details": "Audiobiographical memory encoding; in",
      "DMSP software": "smartphone application for synchronisation of behavioral and neural data streams",
      "Collect data?": "Yes",
      "Data type": "Neural , Wearable sensor, Eye-tracking, \"Invasive (e.g. single unit, ECoG)\", Human, Physiological, Self-report, Audio, Video, Behavioral",
      "Data details": "Audio-visual, accelerometry, GPS, subjective report, autonomic physiology, eye-tracking, NeuroPace Responsive Neurostimulation System",
      "DMSP data": "synchronised behavioral and intracranial EEG; behavioral- recordings from wearable sensors (video, audio, location GPS, accelerometry, eye tracking, psychophysiology)",
      "Challenges (kickoff videos)": "1.\tSynchronization and annotation of (multi-modal) data from the wearable device. (tools to annotate video data from behavior data)\n2.\tAI models for segmenting the environment, LLMs for describing the environment (all in real-time)",
      "Proposed WGs": "Instrumentation, Data, Ethics",
      "Which types of data are you collecting for BBQS?": "Neural data (e.g., EEG, MEG, fMRI, ECoG, single-unit recordings);Behavioral data (e.g., video recordings, motion capture, eye tracking, gait analysis);Physiological data (e.g., heart rate, skin conductance, respiratory rate);Environmental data (e.g., ambient light, temperature, sound levels);Wearable sensor data (e.g., accelerometers, gyroscopes, smartwatches)",
      "Which types of data are you collecting for BBQS?\n(Dropdown)": "Neural data, Behavioral data, Physiological data, Environmental data, Wearable sensor data",
      "What types of behavior are you interested in?": "Locomotion and Movement (for example: walking, running, swimming, climbing, jumping);Eye Movements (for example: saccades, fixations, smooth pursuit);Cognitive and Memory Tasks (for example: problem-solving, decision-making, memory recall);Exploratory Behavior (for example: novel object interaction, maze navigation, environmental exploration);Emotional and Stress Responses (for example: anxiety-related behaviors, fear responses, stress-induced behaviors)",
      "What types of behavior are you interested in? \n(high level)": "Locomotion and Movement, Eye Movements, Cognitive and Memory Tasks, Exploratory Behavior, Emotional and Stress Responses",
      "What types of behavior are you interested in? \n(high level; dropdown)": "Locomotion and Movement, Eye Movements, Cognitive and Memory Tasks, Exploratory Behavior, Emotional and Stress Responses",
      "What statistical or computational methods do you primarily rely on for your data analysis?": "Classical statistical methods (e.g., t-tests, ANOVA);Time series analysis;Unsupervised machine learning (e.g., clustering, PCA);Supervised machine learning (e.g., classification, regression);Deep learning (e.g., neural networks, CNNs, RNNs)\t",
      "What types of analyses are you performing on your neural and behavioral data?": "Statistical analysis, Signal processing, Dimensionality reduction, Time-frequency analysis, Bayesian inference, Correlation analysis, Regression models, Other"
    },
    {
      "award number": "R61MH135114",
      "Type": "R61 / R33",
      "Link": "RFA-MH-22-240",
      "#": "MH22-240",
      "PoP years (max)": "5",
      "Title": "Integrated movement tracking for pediatric OPM-MEG studies of intellectual disability",
      "Award #": "R61MH135114",
      "PI Name": "ROBERTS, TIMOTHY P",
      "PI Email": "robertstim@chop.edu",
      "Human / Animal": "Human",
      "Species (Regular Name)": "Human",
      "Species (Scientific Name)": "Homo sapiens",
      "Sensors": "Optically pumped magnetometers, Movement diagnostic devices",
      "Data Modalities": "Source localization, Evoked responses, Coherence data, Movement kinematics, Video",
      "Approaches": "Profound autism learning, Coherence studies, Eye blink conditioning, Diagnostic tests, Magnetometry",
      "Sensors (Standardized)": "optically pumped magnetometers; video cameras",
      "Sensors\n(Dropdown)": "optically pumped magnetometers, video cameras",
      "Data Modalities (Standardized)": "neural; visual; kinematic",
      "Data Modalities \n(Dropdown)": "neural, visual, kinematic",
      "Approaches (Standardized)": "magnetometry; coherence analysis",
      "Approaches \n(Dropdown)": "magnetometry, coherence analysis",
      "Data Generation Start": "8/1/25",
      "Proposal objective": "\"This R61/R33 project will develop an advanced technology for non-invasive recording of whole-brain physiology with synchronized video-tracking of movement for use in children with intellectual disability and will use it to elucidate the brain-circuit electrophysiology of intellectual development.\"",
      "Build hardware?": "Yes",
      "Hardware type": "Recording device",
      "Hardware details": "\"wearable\" on-scalp OPM-MEG with synchronised movement video tracking",
      "Build software?": "Yes",
      "Software type": "Analysis, Behavioral quantification, Synchronization, Prediction",
      "Software details": "Integration of synchronized video-tracking with MEG; noise cancelling techniques; predict ID during tasks; brain coherence analysis (define frequency bandwidths and brain locations)",
      "DMSP software": "use MNE-python; analysis",
      "Collect data?": "Yes",
      "Data type": "Neural , Human, Non-invasive, Clinical evaluation",
      "Data details": "MEG, movement video tracking, clinical evaluations",
      "DMSP data": "demographic, neuropsychological and diagnostic assessments, EBC, MEG, movement videography, MEG, EBC, video-based derivative dependent variables. ",
      "Challenges (kickoff videos)": "1.\tDevelopment of a wearable OPM-MEG device that can fit on a child\u2019s head \n2.\tDevelopment of noise-canceling techniques to allow head-free movement\n3.\tIntegration and synchronization of video tracking of movement to brain coherence",
      "Proposed WGs": "Instrumentation ",
      "Which types of data are you collecting for BBQS?": "Neural data (e.g., EEG, MEG, fMRI, ECoG, single-unit recordings);Behavioral data (e.g., video recordings, motion capture, eye tracking, gait analysis);Physiological data (e.g., heart rate, skin conductance, respiratory rate);Environmental data (e.g., ambient light, temperature, sound levels);Wearable sensor data (e.g., accelerometers, gyroscopes, smartwatches)",
      "Which types of data are you collecting for BBQS?\n(Dropdown)": "Neural data, Behavioral data, Physiological data, Environmental data, Wearable sensor data",
      "What types of behavior are you interested in?": "Locomotion and Movement (for example: walking, running, swimming, climbing, jumping);Eye Movements (for example: saccades, fixations, smooth pursuit);Cognitive and Memory Tasks (for example: problem-solving, decision-making, memory recall);Exploratory Behavior (for example: novel object interaction, maze navigation, environmental exploration);Emotional and Stress Responses (for example: anxiety-related behaviors, fear responses, stress-induced behaviors)",
      "What types of behavior are you interested in? \n(high level)": "Locomotion and Movement, Eye Movements, Cognitive and Memory Tasks, Exploratory Behavior, Emotional and Stress Responses",
      "What types of behavior are you interested in? \n(high level; dropdown)": "Locomotion and Movement, Eye Movements, Cognitive and Memory Tasks, Exploratory Behavior, Emotional and Stress Responses",
      "What statistical or computational methods do you primarily rely on for your data analysis?": "Classical statistical methods (e.g., t-tests, ANOVA);Time series analysis;Unsupervised machine learning (e.g., clustering, PCA);Supervised machine learning (e.g., classification, regression);Deep learning (e.g., neural networks, CNNs, RNNs)\t",
      "What types of analyses are you performing on your neural and behavioral data?": "Statistical analysis, Signal processing, Dimensionality reduction, Time-frequency analysis, Bayesian inference, Correlation analysis, Regression models, Other"
    },
    {
      "award number": "R61MH135405",
      "Type": "R61 / R33",
      "Link": "RFA-MH-22-240",
      "#": "MH22-240",
      "PoP years (max)": "5",
      "Title": "Developing the Context-Aware Multimodal Ecological Research and Assessment (CAMERA) Platform for Continuous Measurement and Prediction of Anxiety and Memory State",
      "Award #": "R61MH135405",
      "PI Name": "YOUNGERMAN, BRETT E",
      "PI Email": "bey2103@cumc.columbia.edu",
      "Human / Animal": "Human",
      "Species (Regular Name)": "Human",
      "Species (Scientific Name)": "Homo sapiens",
      "Sensors": "iEEG, Audio recording, Smartphone logging; Empatica EmbracePlus",
      "Data Modalities": "EEG data, Smartphone data logging, Video, Audio",
      "Approaches": "Smartphone-based recording, Multimodal sensing, Skin conductance",
      "Sensors (Standardized)": "iEEG; smartphone camera; microphones; eye tracker; heart rate sensors; EDA; skin temperature sensor; accelerometer",
      "Sensors\n(Dropdown)": "iEEG, smartphone camera, microphones, eye tracker, heart rate sensors, EDA, skin temperature sensor",
      "Data Modalities (Standardized)": "neural; physiological; visual; audio",
      "Data Modalities \n(Dropdown)": "neural, physiological, visual, audio",
      "Approaches (Standardized)": "memory analysis; emotion correlation; speech analysis; facial expression analysis; pose estimation; physiological signal analysis; deep learning; ecological momentary assessment",
      "Approaches \n(Dropdown)": "memory analysis, emotion estimation, speech analysis, facial expression analysis, pose estimation, physiological signal analysis, deep learning, ecological momentary assessment",
      "Data Generation Start": "8/1/24",
      "Proposal objective": "\"This project seeks to develop the CAMERA (Context-Aware Multimodal Ecological Research and Assessment) platform, a state-of-the-art open multimodal hardware/software system for measuring human brain\u2013behavior relationships. \"",
      "Build hardware?": "Yes",
      "Hardware type": "Recording device",
      "Hardware details": "Sensors: inctracranial and scalp EEG, smartphone behaviour, physiological tracker, audiovisual tracking; EMA ipad",
      "Build software?": "Yes",
      "Software type": "Tracking, ML model, Analysis, Prediction, Behavioral segmentation, Synchronization, End product",
      "Software details": "use ML to predict memory/anxiety states based on biomarkers; modify with neuromodulation; deep learning for anxiety-memory prediction",
      "DMSP software": "multimodal data processing for analysing passively recorded physiological data including deep learning models for audiovisual processing. software for networking across different devices in data collection and for interfacing with proprietary software.",
      "Collect data?": "Yes",
      "Data type": "Neural , Physiological, Behavioral, Environmental, Multi-modal, Audio, Video, Self-report, Human, Wearable sensor, Clinical evaluation",
      "Data details": "hippocampal theta oscillations, PSD, cross-frequency phase coupling; linguistic features from text input; cardiac, electrodermal activity, skin temp; vocal activity, facial expression, posture, body temp, DL to track pose; ecologiucal momentary assessment (ipad)",
      "DMSP data": "multisensor wristband: primary data: cardiac activity (optical photoplethysmography) (heartrate, high frequency power, skin conductance level (electrodermal activity)). secondary data: systolic peaks, pulse rate variability, respiratory rate, wrist accelerometer, gyroscope motion intensity (sleep, steps, activity), peripheral skin temperature.\n\nsmartphone digital phenotyping: primary: NLP of typed language for linguistic features. secondary: accelerometer/multi-sensor-sleep measures, call log number/duration, app/screen time, environment (ambient light intensity), GPS.\n\naudiovisual recordings: primary: vocal activity (fundamental frequency, standard deviation), environment (ambient noise, presence of another person in room). secondary: vocal activity (loudness, jitter/shimmer, speech rate, NLP/linguistic features), camera (facial activation units-expression, blink magnitude, body movement/posture/gestures), infrared camera (focal body temperature, respiratory rate).\n\nintracranial EEG: primary: hippocampal oscillations in theta and high-frequency bands. secondary: power spectral density, cross frequency coupling and coherence between all sampled structures\n\necological momentary assessment: self-report of anxiety, behavioral logs of activities during spatial memory task, EMG recordings during startle responses\n\ndemographic, neuroimaging, clinical characteristics",
      "Challenges (kickoff videos)": "1.\tIdentification of neural and non-neural biomarkers of memory and anxiety\n2.\tCollection of large data for memory and anxiety in the human brain (hippocampal theta oscillation 3-8 Hz)\n3.\tCollection of behavior data using smartphone (linguistic features), audiovisual tracking (movement, posture, etc.), and physiological sensors (cardiac activity, electrodermal activity, skin temperature). Synchronize all of this using BCI2000 into a single signal stream.\n4.\tML/DL for pose estimation, memory/anxiety detection\n5.\tSynchronization of the single stream signal with Ecological Momentary Assessments (EMAs) i.e. surveys",
      "Proposed WGs": "Instrumentation, Data, Ethics, Deep Learning",
      "Which types of data are you collecting for BBQS?": "Neural data (e.g., EEG, MEG, fMRI, ECoG, single-unit recordings);Behavioral data (e.g., video recordings, motion capture, eye tracking, gait analysis);Physiological data (e.g., heart rate, skin conductance, respiratory rate);Environmental data (e.g., ambient light, temperature, sound levels);Wearable sensor data (e.g., accelerometers, gyroscopes, smartwatches)",
      "Which types of data are you collecting for BBQS?\n(Dropdown)": "Neural data, Behavioral data, Physiological data, Environmental data, Wearable sensor data",
      "What types of behavior are you interested in?": "Locomotion and Movement (for example: walking, running, swimming, climbing, jumping);Eye Movements (for example: saccades, fixations, smooth pursuit);Cognitive and Memory Tasks (for example: problem-solving, decision-making, memory recall);Exploratory Behavior (for example: novel object interaction, maze navigation, environmental exploration);Emotional and Stress Responses (for example: anxiety-related behaviors, fear responses, stress-induced behaviors)",
      "What types of behavior are you interested in? \n(high level)": "Locomotion and Movement, Eye Movements, Cognitive and Memory Tasks, Exploratory Behavior, Emotional and Stress Responses",
      "What types of behavior are you interested in? \n(high level; dropdown)": "Locomotion and Movement, Eye Movements, Cognitive and Memory Tasks, Exploratory Behavior, Emotional and Stress Responses",
      "What statistical or computational methods do you primarily rely on for your data analysis?": "Classical statistical methods (e.g., t-tests, ANOVA);Time series analysis;Unsupervised machine learning (e.g., clustering, PCA);Supervised machine learning (e.g., classification, regression);Deep learning (e.g., neural networks, CNNs, RNNs)\t",
      "What types of analyses are you performing on your neural and behavioral data?": "Statistical analysis, Signal processing, Dimensionality reduction, Time-frequency analysis, Bayesian inference, Correlation analysis, Regression models, Other"
    },
    {
      "award number": "R61MH135407",
      "Type": "R61 / R33",
      "Link": "RFA-MH-22-240",
      "#": "MH22-240",
      "PoP years (max)": "5",
      "Title": "Novel multimodal neural, physiological, and behavioral sensing and machine learning for mental states",
      "Award #": "R61MH135407",
      "PI Name": "SHANECHI, MARYAM",
      "PI Email": "shanechi@usc.edu",
      "Human / Animal": "Human",
      "Species (Regular Name)": "Human",
      "Species (Scientific Name)": "Homo sapiens",
      "Sensors": "iEEG, Physiological data collectors",
      "Data Modalities": "Behavioral data, EEG data",
      "Approaches": "Multimodal sensing and machine learning for mental states",
      "Sensors (Standardized)": "iEEG; heart rate sensors; EDA; accelerometer; cortisol wearable",
      "Sensors\n(Dropdown)": "iEEG, heart rate sensors, EDA, accelerometer, cortisol wearable",
      "Data Modalities (Standardized)": "neural; physiological; visual; audio",
      "Data Modalities \n(Dropdown)": "neural, physiological, visual, audio",
      "Approaches (Standardized)": "speech analysis; facial expression analysis; mental state estimation; multimodal analysis; deep learning",
      "Approaches \n(Dropdown)": "speech analysis, facial expression analysis, mental state estimation, multimodal analysis, deep learning",
      "Data Generation Start": "1/1/25",
      "Proposal objective": "\"The R61 in years 1- 4 will develop and validate the tools in healthy subjects (Aims 1,2) and in epilepsy patients with already-implanted iEEG electrodes which cover many regions related to mental states (Aim 3). In R61, we develop i) an integrated wearable skin-like sensor for multimodal physiological, biomechanical, and cortisol sensing; ii) conversational virtual humans to evoke naturalistic social processes and enable emotion recognition using multimodal audio- visual-language modalities; and iii) a nonlinear, multimodal, brain-behavior modeling, learning, and inference framework for mental states.\"",
      "Build hardware?": "Yes",
      "Hardware type": "Recording device",
      "Hardware details": "wearable skin-like multimodal sensor",
      "Build software?": "Yes",
      "Software type": "Analysis, ML model, Tracking, VR/AR",
      "Software details": "Conversational virtual human agents; continuous audiovisual tracking of mental states from videos; multimodal RNN to decode",
      "DMSP software": "open-source tools; analysis",
      "Collect data?": "Yes",
      "Data type": "Neural , \"Invasive (e.g. single unit, ECoG)\", Wearable sensor, Multi-modal, Physiological, Biochemical, Visual, Audio, Behavioral, Self-report",
      "Data details": "conversational virtual human - multimodal audiovisual language modalities; heart rate, skin conductance, movement, cortisol from sweat; intracranial recording from corticolimbic regions",
      "DMSP data": "Heart rate variability, cortisol level, skin conductance, self-report; intracranial EEG, audiovisual, MRI. ",
      "Challenges (kickoff videos)": "1.\tIntracranial recordings from epilepsy patients\n2.\tDevelopment of virtual conversational agents to evoke naturalistic social processes and continuous audiovisual tracking of mental states from subject videos (behavioral monitoring)\n3.\t(Physiological monitoring) Development of a skin-like multimodal sensor that can monitor body temperature, skin conductance, movement, and cortisol from sweat.",
      "Proposed WGs": "Instrumentation, Deep Learning",
      "Which types of data are you collecting for BBQS?": "Neural data (e.g., EEG, MEG, fMRI, ECoG, single-unit recordings);Behavioral data (e.g., video recordings, motion capture, eye tracking, gait analysis);Physiological data (e.g., heart rate, skin conductance, respiratory rate);Environmental data (e.g., ambient light, temperature, sound levels);Wearable sensor data (e.g., accelerometers, gyroscopes, smartwatches)",
      "Which types of data are you collecting for BBQS?\n(Dropdown)": "Neural data, Behavioral data, Physiological data, Environmental data, Wearable sensor data",
      "What types of behavior are you interested in?": "Locomotion and Movement (for example: walking, running, swimming, climbing, jumping);Eye Movements (for example: saccades, fixations, smooth pursuit);Cognitive and Memory Tasks (for example: problem-solving, decision-making, memory recall);Exploratory Behavior (for example: novel object interaction, maze navigation, environmental exploration);Emotional and Stress Responses (for example: anxiety-related behaviors, fear responses, stress-induced behaviors)",
      "What types of behavior are you interested in? \n(high level)": "Locomotion and Movement, Eye Movements, Cognitive and Memory Tasks, Exploratory Behavior, Emotional and Stress Responses",
      "What types of behavior are you interested in? \n(high level; dropdown)": "Locomotion and Movement, Eye Movements, Cognitive and Memory Tasks, Exploratory Behavior, Emotional and Stress Responses",
      "What statistical or computational methods do you primarily rely on for your data analysis?": "Classical statistical methods (e.g., t-tests, ANOVA);Time series analysis;Unsupervised machine learning (e.g., clustering, PCA);Supervised machine learning (e.g., classification, regression);Deep learning (e.g., neural networks, CNNs, RNNs)\t",
      "What types of analyses are you performing on your neural and behavioral data?": "Statistical analysis, Signal processing, Dimensionality reduction, Time-frequency analysis, Bayesian inference, Correlation analysis, Regression models, Other"
    },
    {
      "award number": "R61MH138966",
      "Type": "R61 / R33",
      "Link": "RFA-MH-22-240",
      "#": "MH22-240",
      "PoP years (max)": "5",
      "Title": "A naturalistic multimodal platform for capturing brain-body interactions in people during physical effort-based decision making",
      "Award #": "R61MH138966",
      "PI Name": "ROZELL, CHRISTOPHER JOHN",
      "PI Email": "crozell@gatech.edu",
      "Human / Animal": "Human",
      "Species (Regular Name)": "Human",
      "Species (Scientific Name)": "Homo sapiens",
      "Sensors": "iEEG, Physiological data collectors",
      "Data Modalities": "Behavioral data, EEG data",
      "Approaches": "Multimodal sensing and machine learning for mental states",
      "Sensors (Standardized)": "iEEG; heart rate sensors; EDA; accelerometer; cortisol wearable",
      "Sensors\n(Dropdown)": "iEEG, heart rate sensors, EDA, accelerometer, cortisol wearable",
      "Data Modalities (Standardized)": "neural; physiological; visual; audio",
      "Data Modalities \n(Dropdown)": "neural, physiological, visual, audio",
      "Approaches (Standardized)": "speech analysis; facial expression analysis; mental state estimation; multimodal analysis; deep learning",
      "Approaches \n(Dropdown)": "speech analysis, facial expression analysis, mental state estimation, multimodal analysis, deep learning",
      "Data Generation Start": "1/1/25",
      "Proposal objective": "\"The project aims to develop a new experimental platform to study naturalistic effort-based decision-making (EBDM) requiring physical effort while simultaneously measuring behavior across decision-making, embodied, affective and clinical domains.\"",
      "Build hardware?": "Yes",
      "Hardware type": "Testing room",
      "Hardware details": "Design and build the EBDM task in virtual environment requiring effortful locomotion",
      "Build software?": "Yes",
      "Software type": "Analysis, Behavioral quantification, VR/AR, Synchronization",
      "Software details": "latent variable models to characterize multimodal brain-body interactions",
      "DMSP software": "analysis; open-source tools",
      "Collect data?": "Yes",
      "Data type": "Neural , Human, Multi-modal, \"Invasive (e.g. single unit, ECoG)\", Behavioral, Clinical evaluation, Audio, Non-invasive",
      "Data details": "behavior across decision-making, embodied, affective, and clinical domains; intracortical electrophysiology; psychomotor, interoceptive and cognitive symptoms at different timescales",
      "DMSP data": "multimodal physiological data: EEG, EMG, ECG (electrocardiogram), SCG (seismocardiogram), PPG (photoplethysmogram); motion capture videos, force plates, respiration signals.\ndaily assessments (CAT-MH); LFP neural recording, neurostimulator; imaging (preop structural, functional and diffusion MRI, postop CT); audiovisual recordings (video diaries); clinical assessments; questionnaires and survey assessments ",
      "Challenges (kickoff videos)": "1.\tIntracranial recordings from epilepsy patients\n2.\tDevelopment of virtual conversational agents to evoke naturalistic social processes and continuous audiovisual tracking of mental states from subject videos (behavioral monitoring)\n3.\t(Physiological monitoring) Development of a skin-like multimodal sensor that can monitor body temperature, skin conductance, movement, and cortisol from sweat.",
      "Proposed WGs": "Instrumentation, Deep Learning",
      "Which types of data are you collecting for BBQS?": "Neural data (e.g., EEG, MEG, fMRI, ECoG, single-unit recordings);Behavioral data (e.g., video recordings, motion capture, eye tracking, gait analysis);Physiological data (e.g., heart rate, skin conductance, respiratory rate);Environmental data (e.g., ambient light, temperature, sound levels);Wearable sensor data (e.g., accelerometers, gyroscopes, smartwatches)",
      "Which types of data are you collecting for BBQS?\n(Dropdown)": "Neural data, Behavioral data, Physiological data, Environmental data, Wearable sensor data",
      "What types of behavior are you interested in?": "Locomotion and Movement (for example: walking, running, swimming, climbing, jumping);Eye Movements (for example: saccades, fixations, smooth pursuit);Cognitive and Memory Tasks (for example: problem-solving, decision-making, memory recall);Exploratory Behavior (for example: novel object interaction, maze navigation, environmental exploration);Emotional and Stress Responses (for example: anxiety-related behaviors, fear responses, stress-induced behaviors)",
      "What types of behavior are you interested in? \n(high level)": "Locomotion and Movement, Eye Movements, Cognitive and Memory Tasks, Exploratory Behavior, Emotional and Stress Responses",
      "What types of behavior are you interested in? \n(high level; dropdown)": "Locomotion and Movement, Eye Movements, Cognitive and Memory Tasks, Exploratory Behavior, Emotional and Stress Responses",
      "What statistical or computational methods do you primarily rely on for your data analysis?": "Classical statistical methods (e.g., t-tests, ANOVA);Time series analysis;Unsupervised machine learning (e.g., clustering, PCA);Supervised machine learning (e.g., classification, regression);Deep learning (e.g., neural networks, CNNs, RNNs)\t",
      "What types of analyses are you performing on your neural and behavioral data?": "Statistical analysis, Signal processing, Dimensionality reduction, Time-frequency analysis, Bayesian inference, Correlation analysis, Regression models, Other"
    },
    {
      "award number": "R61MH138713",
      "Type": "R61 / R33",
      "Link": "RFA-MH-22-240",
      "#": "MH22-240",
      "PoP years (max)": "5",
      "Title": "Combining neural oscillations, physiology and privacy-preserving LiDAR/millimeter wave sensing technology to track attention states in natural contexts",
      "Award #": "R61MH138713",
      "PI Name": "GRAMMER, JENNIE K. ",
      "PI Email": "crozell@gatech.edu",
      "Human / Animal": "Human",
      "Species (Regular Name)": "Human",
      "Species (Scientific Name)": "Homo sapiens",
      "Sensors": "iEEG, Physiological data collectors",
      "Data Modalities": "Behavioral data, EEG data",
      "Approaches": "Multimodal sensing and machine learning for mental states",
      "Sensors (Standardized)": "iEEG; heart rate sensors; EDA; accelerometer; cortisol wearable",
      "Sensors\n(Dropdown)": "iEEG, heart rate sensors, EDA, accelerometer, cortisol wearable",
      "Data Modalities (Standardized)": "neural; physiological; visual; audio",
      "Data Modalities \n(Dropdown)": "neural, physiological, visual, audio",
      "Approaches (Standardized)": "speech analysis; facial expression analysis; mental state estimation; multimodal analysis; deep learning",
      "Approaches \n(Dropdown)": "speech analysis, facial expression analysis, mental state estimation, multimodal analysis, deep learning",
      "Data Generation Start": "1/1/25",
      "Proposal objective": "\"This project aims to develop a portable integrated sensor suite for multidimensional tracking of neuro-behavioral attention states in natural contexts. \"",
      "Build hardware?": "Yes",
      "Hardware type": "Recording device",
      "Hardware details": "Develop a portable, integrated sensor suite for concurrent recording of neural activity, physiological arousal, motor signals, and physical interactions in social environments",
      "Build software?": "Yes",
      "Software type": "Analysis, Prediction, ML model, Synchronization",
      "Software details": "optimize synchronization and portability; privacy-preservation technology to eliminate reliance on video recordings; classification of system states and overall attention states in behavior; predict neuro-behjavioral attention states during different settings",
      "DMSP software": "analysis; open-source tools",
      "Collect data?": "Yes",
      "Data type": "Neural , Physiological, Social interaction, Wearable sensor, Behavioral, Human, Video",
      "Data details": "neural activity, physiological arousal, motor signals, physical interactions in social environments; heart rate, motor, and novel LiDAR/millimeter wave sensing",
      "DMSP data": "EEG signals, gyroscope, accelerometer data, heart rate time course, LiDAR/millimeter wave features (positional and movement); video recordings.",
      "Challenges (kickoff videos)": "1.\tIntracranial recordings from epilepsy patients\n2.\tDevelopment of virtual conversational agents to evoke naturalistic social processes and continuous audiovisual tracking of mental states from subject videos (behavioral monitoring)\n3.\t(Physiological monitoring) Development of a skin-like multimodal sensor that can monitor body temperature, skin conductance, movement, and cortisol from sweat.",
      "Proposed WGs": "Instrumentation, Deep Learning",
      "Which types of data are you collecting for BBQS?": "Neural data (e.g., EEG, MEG, fMRI, ECoG, single-unit recordings), Behavioral data (e.g., video recordings, motion capture, eye tracking, gait analysis), Physiological data (e.g., heart rate, skin conductance, respiratory rate), Cognitive performance data (e.g., reaction times, accuracy, task performance metrics), Self-report data (e.g., questionnaires, diaries, surveys), Wearable sensor data (e.g., accelerometers, gyroscopes, smartwatches)",
      "Which types of data are you collecting for BBQS?\n(Dropdown)": "Neural data, Behavioral data, Physiological data, Cognitive performance data, Self-report data, Wearable sensor data",
      "What types of behavior are you interested in?": "Eye Movements (for example: saccades, fixations, smooth pursuit), Facial Expressions (for example: smiling, frowning, grimacing), Cognitive and Memory Tasks (for example: problem-solving, decision-making, memory recall), Social Interactions (for example: grooming, play behavior, aggression, mating behavior), Emotional and Stress Responses (for example: anxiety-related behaviors, fear responses, stress-induced behaviors), Physical Activity Levels (for example: activity counts, sedentary behavior, exercise routines)",
      "What types of behavior are you interested in? \n(high level)": "Locomotion and Movement, Eye Movements, Cognitive and Memory Tasks, Exploratory Behavior, Emotional and Stress Responses",
      "What types of behavior are you interested in? \n(high level; dropdown)": "Eye Movements, Facial Expressions, Cognitive and Memory Tasks, Social Interactions, Emotional and Stress Responses, Physical Activity Levels",
      "What statistical or computational methods do you primarily rely on for your data analysis?": "Classical statistical methods (e.g., t-tests, ANOVA), Computational models (e.g., Markov chains), Time series analysis, Unsupervised machine learning (e.g., clustering, PCA), Supervised machine learning (e.g., classification, regression), Deep learning (e.g., neural networks, CNNs, RNNs)",
      "What types of analyses are you performing on your neural and behavioral data?": "Statistical analysis, Signal processing, Dynamical systems modeling, Time-frequency analysis, Bayesian inference, Network analysis, Encoding models, Decoding models, Correlation analysis, Regression models"
    },
    {
      "award number": "Contact PD/PI: Flagel, Shelly Tracking Number: GRANT14071262",
      "Type": "U01",
      "Link": "RFA-DA-24-041",
      "#": "DA24-041",
      "PoP years (max)": "5",
      "Title": "to add",
      "Award #": "Contact PD/PI: Flagel, Shelly Tracking Number: GRANT14071262",
      "PI Name": "GRAMMER, JENNIE K. ",
      "PI Email": "crozell@gatech.edu",
      "Human / Animal": "Human",
      "Species (Regular Name)": "Human",
      "Species (Scientific Name)": "Homo sapiens",
      "Sensors": "iEEG, Physiological data collectors",
      "Data Modalities": "Behavioral data, EEG data",
      "Approaches": "Multimodal sensing and machine learning for mental states",
      "Sensors (Standardized)": "iEEG; heart rate sensors; EDA; accelerometer; cortisol wearable",
      "Sensors\n(Dropdown)": "iEEG, heart rate sensors, EDA, accelerometer, cortisol wearable",
      "Data Modalities (Standardized)": "neural; physiological; visual; audio",
      "Data Modalities \n(Dropdown)": "neural, physiological, visual, audio",
      "Approaches (Standardized)": "speech analysis; facial expression analysis; mental state estimation; multimodal analysis; deep learning",
      "Approaches \n(Dropdown)": "speech analysis, facial expression analysis, mental state estimation, multimodal analysis, deep learning",
      "Data Generation Start": "1/1/25",
      "Proposal objective": "\"This project aims to develop a portable integrated sensor suite for multidimensional tracking of neuro-behavioral attention states in natural contexts. \"",
      "Build hardware?": "Yes",
      "Hardware type": "Recording device",
      "Hardware details": "Develop a portable, integrated sensor suite for concurrent recording of neural activity, physiological arousal, motor signals, and physical interactions in social environments",
      "Build software?": "Yes",
      "Software type": "Analysis, Prediction, ML model, Synchronization",
      "Software details": "optimize synchronization and portability; privacy-preservation technology to eliminate reliance on video recordings; classification of system states and overall attention states in behavior; predict neuro-behjavioral attention states during different settings",
      "DMSP software": "analysis; open-source tools",
      "Collect data?": "Yes",
      "Data type": "Neural , Physiological, Social interaction, Wearable sensor, Behavioral, Human, Video",
      "Data details": "neural activity, physiological arousal, motor signals, physical interactions in social environments; heart rate, motor, and novel LiDAR/millimeter wave sensing",
      "DMSP data": "EEG signals, gyroscope, accelerometer data, heart rate time course, LiDAR/millimeter wave features (positional and movement); video recordings.",
      "Challenges (kickoff videos)": "1.\tIntracranial recordings from epilepsy patients\n2.\tDevelopment of virtual conversational agents to evoke naturalistic social processes and continuous audiovisual tracking of mental states from subject videos (behavioral monitoring)\n3.\t(Physiological monitoring) Development of a skin-like multimodal sensor that can monitor body temperature, skin conductance, movement, and cortisol from sweat.",
      "Proposed WGs": "Instrumentation, Deep Learning",
      "Which types of data are you collecting for BBQS?": "Neural data (e.g., EEG, MEG, fMRI, ECoG, single-unit recordings), Behavioral data (e.g., video recordings, motion capture, eye tracking, gait analysis), Physiological data (e.g., heart rate, skin conductance, respiratory rate), Cognitive performance data (e.g., reaction times, accuracy, task performance metrics), Self-report data (e.g., questionnaires, diaries, surveys), Wearable sensor data (e.g., accelerometers, gyroscopes, smartwatches)",
      "Which types of data are you collecting for BBQS?\n(Dropdown)": "Neural data, Behavioral data, Physiological data, Cognitive performance data, Self-report data, Wearable sensor data",
      "What types of behavior are you interested in?": "Eye Movements (for example: saccades, fixations, smooth pursuit), Facial Expressions (for example: smiling, frowning, grimacing), Cognitive and Memory Tasks (for example: problem-solving, decision-making, memory recall), Social Interactions (for example: grooming, play behavior, aggression, mating behavior), Emotional and Stress Responses (for example: anxiety-related behaviors, fear responses, stress-induced behaviors), Physical Activity Levels (for example: activity counts, sedentary behavior, exercise routines)",
      "What types of behavior are you interested in? \n(high level)": "Locomotion and Movement, Eye Movements, Cognitive and Memory Tasks, Exploratory Behavior, Emotional and Stress Responses",
      "What types of behavior are you interested in? \n(high level; dropdown)": "Eye Movements, Facial Expressions, Cognitive and Memory Tasks, Social Interactions, Emotional and Stress Responses, Physical Activity Levels",
      "What statistical or computational methods do you primarily rely on for your data analysis?": "Classical statistical methods (e.g., t-tests, ANOVA), Computational models (e.g., Markov chains), Time series analysis, Unsupervised machine learning (e.g., clustering, PCA), Supervised machine learning (e.g., classification, regression), Deep learning (e.g., neural networks, CNNs, RNNs)",
      "What types of analyses are you performing on your neural and behavioral data?": "Statistical analysis, Signal processing, Dynamical systems modeling, Time-frequency analysis, Bayesian inference, Network analysis, Encoding models, Decoding models, Correlation analysis, Regression models"
    }
  ]
}